<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>promise</title>
      <link href="/2022/05/25/about-promise/"/>
      <url>/2022/05/25/about-promise/</url>
      
        <content type="html"><![CDATA[<h1 id="Promise知识点梳理"><a href="#Promise知识点梳理" class="headerlink" title="Promise知识点梳理"></a>Promise知识点梳理</h1><p>之前对promise进行过一定的了解，但是仍然对其中部分内容理解不够全面，今天就针对Promise进行一下小小的总结。</p><h2 id="Promise的出现"><a href="#Promise的出现" class="headerlink" title="Promise的出现"></a>Promise的出现</h2><p>ECMAScript6及之后的几个版本加大了对异步编程机制的支持，而Promise引用类型就是ECMAScript6因此新增的类型，使得优雅地定义和组织异步逻辑成为现实，而且在此之后地几个版本更是增加了使用async和await关键字定义异步函数的机制。</p><h3 id="异步编程"><a href="#异步编程" class="headerlink" title="异步编程"></a>异步编程</h3><p>JavaScript是一种单线程事件循环模型，同步操作和异步操作是代码所要依赖的核心机制，异步行为是为了优化因计算量大而时间长的操作（其实当我们不想因为某个异步操作而阻塞线程执行时，都可以使用异步编程）</p><h3 id="同步和异步"><a href="#同步和异步" class="headerlink" title="同步和异步"></a>同步和异步</h3><ul><li>同步行为对应内存中顺序执行的处理器指令，每条指令都会严格按照它们出现的顺序来执行，每条指令执行后也能立即存储在系统本地（例如寄存器或者系统内存）</li><li>异步行为类似于系统中断，即当前进程外部的实体可以触发代码执行，异步操作经常是必要的，因为强制进程等待一个长时间的操作是不可行的（同步操作必须要等），如果代码要访问一些高延迟的资源，例如向远程服务器发送请求并等待响应，那么就会出现长时间的等待。另外，到底什么时候会触发这个中断，这对JavaScript运行时来说是一个黑盒，在排定回调之后基本无法知道系统状态何时变化。设计一个能够在异步行为完成后通知其他代码的系统是非常难的，JavaScript在实现这样一个系统的过程中也经历了几次迭代。</li></ul><h3 id="Promise前的异步编程模式"><a href="#Promise前的异步编程模式" class="headerlink" title="Promise前的异步编程模式"></a>Promise前的异步编程模式</h3><p>在早期的JavaScript中，支支持定义回调函数来表明异步操作完成，串联多个异步操作是一个常见的问题，通常需要深度嵌套的回调函数（即“回调地狱”）来解决  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">function double(value,success,failure)&#123;</span><br><span class="line">    setTimeout(()=&gt;&#123;</span><br><span class="line">        try&#123;</span><br><span class="line">            if(typeof value !==&#x27;number&#x27;)&#123;</span><br><span class="line">                throw &#x27;Must provide number as first argument&#x27;</span><br><span class="line">            &#125;</span><br><span class="line">            success(2*value)</span><br><span class="line">        &#125;catch(e)&#123;</span><br><span class="line">            failure(e)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;,1000)</span><br><span class="line">&#125;</span><br><span class="line">const successCallback = (x)=&gt;&#123;</span><br><span class="line">    double(x,(y)=&gt;console.log(`Success:$&#123;y&#125;`))</span><br><span class="line">&#125;</span><br><span class="line">const faliureCallback = (e) =&gt; &#123;</span><br><span class="line">    console.log(`Failure:$&#123;e&#125;`)</span><br><span class="line">&#125;</span><br><span class="line">double(3,successCallback,failureCallback)  </span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">以上为回调嵌套的一个实例，大约2000ms之后输出Success:12   </span><br><span class="line">可以看出上述代码看上去还是比较复杂的</span><br><span class="line"></span><br><span class="line">### Promise</span><br><span class="line">于是就出现了Promise,翻译成中文是“期约”，这是对尚不存在结果的一个替身。这个名字在1976年就被提出，到了2010年，CommonJS项目实现的Promises/A规范日益流行，2012年Promises/A+组织fork了CommonJS的Promises/A建议，并以相同的名字制定了Promises/A+规范，成为了ECMAScript6规范实现的范本。  </span><br><span class="line"></span><br><span class="line">## Promise基础</span><br><span class="line">### Promise状态  </span><br><span class="line">pending  -&gt; fulfilled  </span><br><span class="line">pending  -&gt; rejected  </span><br><span class="line">而且Promise的状态是私有的，不能直接通过JavaScript检测到，也不能被外部JavaScript代码修改，这样可以避免根据读取到的Promise状态，以同步方式处理Promise对象</span><br><span class="line"></span><br><span class="line">### new Promise()的过程</span><br><span class="line">这块跟手写Promise有点关系，理解了使用Promise之后整个的工作流程对于手写Promise比较有帮助  </span><br><span class="line">下面看一个(伪)代码实例    </span><br><span class="line"></span><br><span class="line">```  </span><br><span class="line">let p = new Promise((resolve,reject)=&gt;&#123;</span><br><span class="line">    console.log(&#x27;initial promise!&#x27;)</span><br><span class="line">    /*</span><br><span class="line">    可以完成各种异步任务,并根据异步任务的返回结果使用resolve()或者reject()函数把promise的状态变为fulfilled或者rejected</span><br><span class="line">    */</span><br><span class="line">    if( 异步任务成功执行)&#123; </span><br><span class="line">        resolve(111)</span><br><span class="line">    &#125;</span><br><span class="line">    else&#123;</span><br><span class="line">        reject(&#x27;error&#x27;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">p.then((value)=&gt;&#123;console.log(value)&#125;,(error)=&gt;console.log(error))  </span><br></pre></td></tr></table></figure><ol><li>在创建promise实例时，传入的参数为一个执行器函数。这个函数有两个参数，分别是resolve和reject，这两个函数用来改变promise的状态，而且promise的状态只能由pending-&gt;fulfilled/rejected改变一次，改变之后就不能再改变；在执行器函数中可以完成异步任务，并根据异步任务的返回结果调用resolve()或者reject()  </li><li>Promise.prototype中定义了.then()  .catch() .finally()等方法，作用都是类似的，指定了Promise实例的成功或失败回调，当Promise实例的状态发生改变时就会调用对应的回调函数</li></ol><h2 id="Promise中resolve-与-then-的关系"><a href="#Promise中resolve-与-then-的关系" class="headerlink" title="Promise中resolve()与.then()的关系"></a>Promise中resolve()与.then()的关系</h2><p>这块内容我之前一直有一些琢磨不透，理解这两个函数对于我们理解Promise实现整个异步过程非常有帮助，在查了一些资料之后，我认为应该是以下的流程，这样我也就理解了为什么在Promise状态改变前后在.then()中注册的回调函数都可以被执行，另外就是rejected()和resolve()是完全一样的，反过来去理解就行  </p><ol><li>resolve()方法  </li></ol><ul><li>如果Promise是pending，则将状态改为fulfilled</li><li>将Promise的value赋值为resolve参数</li><li>如果有onFulfilledCallback,则将其推到微队列中  </li></ul><ol start="2"><li>then()方法  </li></ol><ul><li>如果Promise是pending状态：<br>(1) 将then的onFulfilled方法追加保存到Promise的onFulfilledCallback，即规范中的PromiseFulfilledReactions列表末尾；<br>(2) 将then的onejected方法追加保存到Promise的onRejectedCallback，即规范中的PromiseRejectedReactions列表末尾；  </li><li>如果Promise的状态是fulfilled状态，则把onFulfilled推到微任务队列  </li><li>如果Promise的状态是rejected状态，则把onRejected推到微任务队列中  </li></ul><p>这里再提醒一点，其实new Promise()和.then()都是同步的，只不过在实例化Promise对象的过程中使用的执行器函数中是可以放异步代码的，然后通过上述机制实现这个异步效果   </p><p>下面看一个关于这个知识点的代码  </p><pre><code>// 1. 把定时器任务推到宏队列中// 13.执行宏队列，打印 0setTimeout(() =&gt; &#123;  console.log(0)&#125;, 0);// 2. 调用 resolve，此时 p1 的状态变为 fulfilledlet p1 = new Promise((resolve, reject) =&gt; &#123;  resolve(&#123;name: 1&#125;)&#125;)// 3. 执行 then，把 onFulfilled 推到微队列// 9. 执行微队列，打印 1// 9. p2 状态变为 fulfilled, 将 p2 的 onFulfilledCallback 推到微队列let p2 = p1.then(  () =&gt; &#123;    console.log(1)  &#125;)// 4. 执行 then，p2 是 pending 状态，所以保存 onFulfulled 到 P2 的 onFulfilledCallback 中// 11. 执行微队列，打印 2let p3 = p2.then(value =&gt; &#123;  console.log(2)&#125;)// 5. 调用 resolve，此时 p4 的状态变为 fulfilledlet p4 = new Promise((resolve, reject) =&gt; &#123;    resolve(&#39;Promise&#39;)&#125;)// 6. 执行 then，把 onFulfilled 推到微队列// 10. 执行微队列，打印 3// 10. p5 状态变为 fulfilled, 将 p5 的 onFulfilledCallback 推到微队列let p5 = p4.then(  ()=&gt; &#123;    console.log(3)  &#125;,  error =&gt; console.log(error))// 7. 执行 then，p5 是 pending 状态，所以保存 onFulfulled 到 p5 的 onFulfilledCallback 中// 12. 执行微队列，打印 4let p6 = p5.then(value =&gt; &#123;  console.log(4)&#125;)// 8. 输出 5console.log(5)</code></pre><p>结果是5，1，3，2，4，0</p>]]></content>
      
      
      <categories>
          
          <category> 前端学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> js原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>react简介</title>
      <link href="/2022/05/08/react1/"/>
      <url>/2022/05/08/react1/</url>
      
        <content type="html"><![CDATA[<h1 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h1><p>React是一个用于构建用户界面的js库，是MVC中的V起源于Facebook内部项目，代码逻辑非常简单。</p><h1 id="与Vue的区别"><a href="#与Vue的区别" class="headerlink" title="与Vue的区别"></a>与Vue的区别</h1><p>待更新，等学完react之后进行总结更新</p>]]></content>
      
      
      <categories>
          
          <category> 前端学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HTML-iframe元素</title>
      <link href="/2022/05/07/%E7%9F%A5%E8%AF%86%E7%82%B91/"/>
      <url>/2022/05/07/%E7%9F%A5%E8%AF%86%E7%82%B91/</url>
      
        <content type="html"><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>iframe元素会创建包含另外一个文档的内联框架（即行内框架，在当前HTML页面中嵌入另一个文档）</p><h1 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;iframe src = &quot;文件路径&quot;&gt; &lt;/iframe&gt;</span><br></pre></td></tr></table></figure><h2 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h2><ul><li>height</li><li>width</li><li>name 定义框架名称</li><li>frameborder  是否需要显示边框，定义为1表示需要边框</li><li>scrolling  值可以是yes,no或auto</li><li>src</li><li>align  值可以是left,right,top,middle,bottom</li></ul><h1 id="iframe优缺点"><a href="#iframe优缺点" class="headerlink" title="iframe优缺点"></a>iframe优缺点</h1><ul><li>优点<ol><li>页面和代码分离，几乎不会受到外界CSS和js的影响，便于使用；</li><li>代码复用；</li><li>重新加载页面时不需要重载iframe框架页内容，增加重载速度；</li><li>可以解决第三方内容加载缓慢的问题；</li><li>可以实现跨子域通信；</li><li>可以使脚本并行下载（iframe可以与主页面的其他元素并行下载而不会阻塞，iframe包含了其他的html页面，而html页面也可以包含js脚本，因此可以将需要加载的js脚本放入一个html文件中然后使用iframe非阻塞加载这个html文件）。</li></ol></li><li>缺点<ol><li>页面过多不易管理；</li><li>iframe不会被搜索引擎捕获，不利于SEO；</li><li>iframe兼容性较差；</li><li>iframe有一定的安全风险；</li><li>iframe会阻塞主页面的onLoad事件</li></ol></li></ul><h1 id="iframe和frame的区别"><a href="#iframe和frame的区别" class="headerlink" title="iframe和frame的区别"></a>iframe和frame的区别</h1><ol><li>frame不能frameSet单独存在，iframe可以；</li><li>frame不能放在body中，iframe可以；</li><li>嵌套在frameSet中的iframe必须放在body中；</li><li>frame的高度只能由frameSet控制，iframe可以自己控制，不能通过frameSet控制；</li><li>iframe可以在表格中使用，frame无法使用；</li><li>h5中支持iframe，废弃了frame；</li><li>frame和iframe实现功能基本相同，但iframe更灵活，frame是整个页面的框架，iframe是内嵌的网页元素。</li></ol><h1 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h1><p>当同域时，父页面可以对子页面进行 改写，反之亦然；<br>不同域时，父页面没有权限改动子页面，但是可以实现页面跳转。</p>]]></content>
      
      
      <categories>
          
          <category> html </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 基础知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记录贴：ubuntu之cuda安装（AI平台）</title>
      <link href="/2022/03/18/%E8%AE%B0%E5%BD%95%E8%B4%B4-AI%E5%B9%B3%E5%8F%B0ubuntu%E4%BD%BF%E7%94%A8anaconda%E5%AE%89%E8%A3%85cuda/"/>
      <url>/2022/03/18/%E8%AE%B0%E5%BD%95%E8%B4%B4-AI%E5%B9%B3%E5%8F%B0ubuntu%E4%BD%BF%E7%94%A8anaconda%E5%AE%89%E8%A3%85cuda/</url>
      
        <content type="html"><![CDATA[<h1 id="使用anaconda在Ubuntu20-04安装cuda8-0"><a href="#使用anaconda在Ubuntu20-04安装cuda8-0" class="headerlink" title="使用anaconda在Ubuntu20.04安装cuda8.0"></a>使用anaconda在Ubuntu20.04安装cuda8.0</h1><h2 id="显卡驱动安装"><a href="#显卡驱动安装" class="headerlink" title="显卡驱动安装"></a>显卡驱动安装</h2><ol><li>sudo ubuntu-drivers autoinstall</li><li>sudo reboot</li><li>nvidia-smi  </li></ol><h2 id="cudacnn及cudatoolkit安装"><a href="#cudacnn及cudatoolkit安装" class="headerlink" title="cudacnn及cudatoolkit安装"></a>cudacnn及cudatoolkit安装</h2><ol><li>conda install cudnn=5.1</li><li>conda install cudatoolkit=8.0</li></ol>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生物数据处理(一)</title>
      <link href="/2022/02/18/%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
      <url>/2022/02/18/%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="使用Biopython-SeqIO-读取fasta序列文件、读取序列信息以及写入文件"><a href="#使用Biopython-SeqIO-读取fasta序列文件、读取序列信息以及写入文件" class="headerlink" title="使用Biopython SeqIO 读取fasta序列文件、读取序列信息以及写入文件"></a>使用Biopython SeqIO 读取fasta序列文件、读取序列信息以及写入文件</h1><h2 id="Biopython"><a href="#Biopython" class="headerlink" title="Biopython"></a>Biopython</h2><ol><li>序列赋值 转录（反转录） 翻译 反向互补</li><li>读取序列文件，识别序列的属性信息。SeqRecord提供序列及其注释的容器  </li></ol><h2 id="属性："><a href="#属性：" class="headerlink" title="属性："></a>属性：</h2><ul><li>seq :一条生物序列</li><li>id：基本ID，标识这条序列</li><li>name：常用分子的名称</li><li>description：序列分子的描述</li><li>letter_annotation:是一个有给每个碱基注释的字典，键是注释类型，值是每个残基序列注释的列表</li><li>annotations：序列附件信息的字典。键是信息的类型，值包含信息</li><li>features：是SeqFeature对象的列表</li></ul><h2 id="读取序列文件"><a href="#读取序列文件" class="headerlink" title="读取序列文件"></a>读取序列文件</h2><ol><li>只包含一个序列条目的文件，Bio.SeqIO.read(文件句柄，序列格式) #最新版都可以直接用文件，不过用with 句柄也可以更加规范 如：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from Bio import SeqIO</span><br><span class="line">&gt;&gt;&gt; record = SeqIO.read(&quot;Fasta/f001&quot;, &quot;fasta&quot;)</span><br><span class="line">&gt;&gt;&gt; print(&quot;%s %i&quot; % (record.id, len(record)))</span><br></pre></td></tr></table></figure>gi|3318709|pdb|1A91| 79  </li><li>多条序列的文件，Bio.SeqIO.parse(文件句柄，序列格式),返回SeqRecord 对象迭代器如：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from Bio import SeqIO</span><br><span class="line">for seq_record in SeqIO.parse(&quot;ls_orchid.fasta&quot;, &quot;fasta&quot;):</span><br><span class="line">    print seq_record.id</span><br><span class="line">    print repr(seq_record.seq)</span><br><span class="line">    print len(seq_record)</span><br></pre></td></tr></table></figure>还可以用next() 遍历序列条目,如：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from Bio import SeqIO</span><br><span class="line">record_iterator = SeqIO.parse(&quot;ls_orchid.fasta&quot;, &quot;fasta&quot;)</span><br><span class="line"></span><br><span class="line">first_record = record_iterator.next()</span><br><span class="line">print first_record.id</span><br><span class="line">print first_record.description</span><br><span class="line"></span><br><span class="line">second_record = record_iterator.next()</span><br><span class="line">print second_record.id</span><br><span class="line">print second_record.description</span><br></pre></td></tr></table></figure>保存为序列条目列表 list()即可<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from Bio import SeqIO</span><br><span class="line">records = list(SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;))</span><br><span class="line"></span><br><span class="line">print &quot;Found %i records&quot; % len(records)</span><br><span class="line"></span><br><span class="line">print &quot;The last record&quot;</span><br><span class="line">last_record = records[-1]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="提取序列条目信息"><a href="#提取序列条目信息" class="headerlink" title="提取序列条目信息"></a>提取序列条目信息</h2>annotations 属性 得到序列条目的注释信息,如：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print first_record.annotations</span><br></pre></td></tr></table></figure></li></ol><h2 id="gzip文件提取，先gzip模块得句柄，再操作"><a href="#gzip文件提取，先gzip模块得句柄，再操作" class="headerlink" title="gzip文件提取，先gzip模块得句柄，再操作"></a>gzip文件提取，先gzip模块得句柄，再操作</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import gzip</span><br><span class="line">&gt;&gt;&gt; from Bio import SeqIO</span><br><span class="line">&gt;&gt;&gt; handle = gzip.open(&quot;ls_orchid.gbk.gz&quot;, &quot;r&quot;)</span><br><span class="line">&gt;&gt;&gt; print sum(len(r) for r in SeqIO.parse(handle, &quot;gb&quot;))</span><br></pre></td></tr></table></figure><p>67518</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; handle.close()</span><br></pre></td></tr></table></figure><h2 id="保存序列文件到字典，默认序列id作为key-value是序列的Seq对象"><a href="#保存序列文件到字典，默认序列id作为key-value是序列的Seq对象" class="headerlink" title="保存序列文件到字典，默认序列id作为key,value是序列的Seq对象"></a>保存序列文件到字典，默认序列id作为key,value是序列的Seq对象</h2><p>Bio.SeqIO.to_dict()内存占用大，每个条目以 SeqRecord 对象形式存储在内存中，你修改这些条目。<br>Bio.SeqIO.index() 处于中间水平，类似于只读字典，当需要时解析序列到 SeqRecord 对象。<br>Bio.SeqIO.index() ，工作原理上略有不同。尽管仍然是返回一个类似于字典的对象，它并不将所有的信息存储在内存中。相反，它仅仅记录每条序列条目在文件中的位置 - 当你需要读取某条特定序列条目时，它才进行解析。<br>注：index()只接受文件名，不接受句柄。如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from Bio import SeqIO</span><br><span class="line">&gt;&gt;&gt; orchid_dict = SeqIO.index(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;)</span><br><span class="line">&gt;&gt;&gt; len(orchid_dict)</span><br></pre></td></tr></table></figure><p>94<br>Bio.SeqIO.index_db() 也类似于只读字典，但是将文件中的ID和文件偏移值存储到硬盘（SQLite3数据库），这意味着它对内存需求很低（请见第 5.4.3 节），但会慢一点。<br>如：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from Bio import SeqIO</span><br><span class="line">&gt;&gt;&gt; orchid_dict = SeqIO.to_dict(SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;))</span><br></pre></td></tr></table></figure><p> 来自 Bio.SeqIO.index() 的字典样对象以 SeqRecord 对象形式返回序列条目。但是，有时候从文件中直接获取原始数据非常有用。对于此种情况，使用 get_raw() 方法获取文件原始数据，方便选择序列再写入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from Bio import SeqIO</span><br><span class="line">&gt;&gt;&gt; uniprot = SeqIO.index(&quot;uniprot_sprot.dat&quot;, &quot;swiss&quot;)</span><br><span class="line">&gt;&gt;&gt; handle = open(&quot;selected.dat&quot;, &quot;w&quot;)</span><br><span class="line">&gt;&gt;&gt; for acc in [&quot;P33487&quot;, &quot;P19801&quot;, &quot;P13689&quot;, &quot;Q8JZQ5&quot;, &quot;Q9TRC7&quot;]:</span><br><span class="line">&gt;&gt;&gt;      handle.write(uniprot.get_raw(acc))</span><br><span class="line">&gt;&gt;&gt; handle.close()</span><br></pre></td></tr></table></figure><h2 id="序列写入，序列文件格式转换"><a href="#序列写入，序列文件格式转换" class="headerlink" title="序列写入，序列文件格式转换"></a>序列写入，序列文件格式转换</h2><p>Bio.SeqIO.write() 输出序列（写入文件），该函数需要三个参数：某些 SeqRecord 对象，要写入的句柄或文件名，和序列格式</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from Bio import SeqIO</span><br><span class="line">records = SeqIO.parse(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;)</span><br><span class="line">count = SeqIO.write(records, &quot;my_example.fasta&quot;, &quot;fasta&quot;)</span><br><span class="line">print &quot;Converted %i records&quot; % count</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from Bio import SeqIO</span><br><span class="line">count = SeqIO.convert(&quot;ls_orchid.gbk&quot;, &quot;genbank&quot;, &quot;my_example.fasta&quot;, &quot;fasta&quot;)</span><br><span class="line">print &quot;Converted %i records&quot; % count</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 生物数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSS</title>
      <link href="/2022/01/30/%E5%89%8D%E7%AB%AF-css/"/>
      <url>/2022/01/30/%E5%89%8D%E7%AB%AF-css/</url>
      
        <content type="html"><![CDATA[<p> CSS 学习教程（一）</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><pre><code>CSS是层叠样式表的简称，也是一种标记语言，在HTML的基础上对页面布局进行美化~CSS主要用于设置HTML页面中文本内容（字体、大小、对齐方式等）、图片的外形（宽高、边框样式、边距等）以及版面的布局和外观显示样式  HTML专注去做结构，样式交给CSS（在&lt;head&gt;标签中添加&lt;style&gt;&lt;/style&gt;标签对）  CSS规则主要由两部分组成：选择器以及一条或者多条声明，即：选择器&#123;属性：属性值；属性：属性值；&#125;</code></pre><h1 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h1><h2 id="基础选择器：由单个选择器组成"><a href="#基础选择器：由单个选择器组成" class="headerlink" title="基础选择器：由单个选择器组成"></a>基础选择器：由单个选择器组成</h2><h3 id="标签选择器"><a href="#标签选择器" class="headerlink" title="标签选择器"></a>标签选择器</h3><p>标签选择器（元素选择器）是指用HTML标签名称作为选择器，按标签名称分类，为页面中某一标签指定统一的CSS样式</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">标签 &#123;</span><br><span class="line">    属性1：属性值1；</span><br><span class="line">    属性2：属性值2；</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="类选择器"><a href="#类选择器" class="headerlink" title="类选择器"></a>类选择器</h3><p>可以实现单独选择某一个或者某几个标签</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.类名&#123;</span><br><span class="line">    属性1：属性值1；</span><br><span class="line">    属性2：属性值2；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同时在要使用该样式的HTML元素中添加class属性为类名<br>多类名：可以给一个标签指定多个类名，从而大导更多的选择目的，通过这些类名都可以选出这个标签，多个类名之间必须以空格分开</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;cover J-Cover&quot;&gt;啦啦啦&lt;/div&gt;</span><br></pre></td></tr></table></figure><h3 id="id选择器"><a href="#id选择器" class="headerlink" title="id选择器"></a>id选择器</h3><p>id选择器可以为具有特定id的HTML元素指定特定的样式<br>HTML元素以id属性值设置id选择器，CSS中以”#”+id的形式定义选择器  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#nav &#123;color: red;&#125;</span><br><span class="line">&lt;div id=&quot;nav&quot;&gt; 啦啦啦&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>与类选择器的区别就是只能被调用一次，id和class可以理解为人的名字和身份证号</p><h3 id="通配符选择器"><a href="#通配符选择器" class="headerlink" title="通配符选择器"></a>通配符选择器</h3><p>在CSS中，通配符选择器使用”*”进行定义，它表示选取页面中所有的元素（标签），不需要调用，默认给所有元素使用样式  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* &#123;属性1： 属性值1；</span><br><span class="line">   属性2： 属性值2；&#125;</span><br></pre></td></tr></table></figure><h2 id="复合选择器"><a href="#复合选择器" class="headerlink" title="复合选择器"></a>复合选择器</h2><p>复合选择器是由两个或者多个基础选择器，通过不同的方式组合而成的包括后代选择器、子选择器、并集选择器和伪类选择器</p><h3 id="后代选择器：父代（元素1）-子代（元素2）-样式声明"><a href="#后代选择器：父代（元素1）-子代（元素2）-样式声明" class="headerlink" title="后代选择器：父代（元素1） 子代（元素2）{ 样式声明}"></a>后代选择器：父代（元素1） 子代（元素2）{ 样式声明}</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;style&gt;</span><br><span class="line">    ol li&#123;</span><br><span class="line">        color: red;</span><br><span class="line">        &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><ul><li>元素1和元素2中间必须用空格分开</li><li>元素2必须是元素1的后代标签.可以是儿子也可以是孙子（会把父代符合条件的所有后代选出来）</li><li>最终选择的是元素2</li><li>也可以有3层，如 ol li a { } </li><li>元素1和元素2可以是任意的基础选择器（上边都是用标签选择器做例子）</li></ul><h3 id="子选择器-：元素1-gt-元素2-样式声明"><a href="#子选择器-：元素1-gt-元素2-样式声明" class="headerlink" title="子选择器 ：元素1 &gt; 元素2 {样式声明}"></a>子选择器 ：元素1 &gt; 元素2 {样式声明}</h3><ul><li>子选择器只能选择某元素最近一级的子元素，简单来说就是亲儿子这一级的元素（与后代选择器不同的一点）</li></ul><h3 id="并集选择器-：元素1-元素2-样式声明"><a href="#并集选择器-：元素1-元素2-样式声明" class="headerlink" title="并集选择器 ：元素1,元素2 {样式声明}"></a>并集选择器 ：元素1,元素2 {样式声明}</h3><ul><li>并集选择器可以选择多组标签，同时为它们定义相同的样式，通常用于集体声明，使用英文逗号进行分割  </li><li>任何形式的选择器（包括前边几种复合选择器）都可以作为并集选择器的一部分</li><li>约定的语法规范：并集选择器习惯竖着写</li><li>一定要注意最后一个选择器不需要加逗号</li></ul><h3 id="伪类选择器"><a href="#伪类选择器" class="headerlink" title="伪类选择器"></a>伪类选择器</h3><ul><li>伪类选择器用于给某些选择器添加特殊的效果，比如给链接添加效果，或者选择第1个或者第n个元素</li><li>伪类选择器最大的特点是用”冒号”表示,比如:hover 以及 ：first-child</li><li>伪类选择器有很多，比如常见的链接伪类、结构伪类、focus伪类等  </li></ul><h4 id="链接伪类选择器"><a href="#链接伪类选择器" class="headerlink" title="链接伪类选择器"></a>链接伪类选择器</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a:link &#123;样式&#125; /*选择所有未被访问过的链接*/</span><br><span class="line">a:visited &#123;&#125; /*选择所有已被访问过的链接*/</span><br><span class="line">a:hover &#123;&#125; /*选择鼠标指针位于其上的链接*/</span><br><span class="line">a:active &#123;&#125; /*选择活动链接（鼠标点下未弹起的链接）*/</span><br></pre></td></tr></table></figure><p>补充一点，a:hover 这样的前边的a不一定必须是标签选择器哦~可以是复合选择器或者id选择器等,而且任何的盒子都可以添加:hover,甚至在:hover后边可以添加进一步的选择器（比如 .tudou:hover .mask{}）<br>链接伪类注意事项:<br>为确保生效，要按照 :link :visited :hover :active的顺序进行书写</p><h4 id="：focus伪类选择器"><a href="#：focus伪类选择器" class="headerlink" title="：focus伪类选择器"></a>：focus伪类选择器</h4><p>用于选择获得焦点的表单元素，一般&lt; input&gt;类表单元素才能获取，因此这个选择器主要针对表单元素</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">input:focus &#123;</span><br><span class="line">    background-color:blue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h1><h2 id="CSS字体属性"><a href="#CSS字体属性" class="headerlink" title="CSS字体属性"></a>CSS字体属性</h2><h3 id="font-family-设置字体-font-family-“微软雅黑”"><a href="#font-family-设置字体-font-family-“微软雅黑”" class="headerlink" title="font-family 设置字体  font-family: “微软雅黑”;"></a>font-family 设置字体  font-family: “微软雅黑”;</h3><ul><li>可以设置多种字体作为属性值，如果是多种字体的话那么各个字体之间要用英文状态下的逗号隔开</li><li>如果由空格隔开的多个单词组成的字体属性值，则要在外边加引号  </li><li>尽量使用系统默认自带的字体，保证在任何浏览器下都能正确显示</li></ul><h3 id="font-size-设置字体大小-font-size-20px"><a href="#font-size-设置字体大小-font-size-20px" class="headerlink" title="font-size 设置字体大小 font-size: 20px;"></a>font-size 设置字体大小 font-size: 20px;</h3><p>不同浏览器可能默认显示的字号大小不一致，最好不要默认大小而应该给一个明确值;标题标签比较特殊，需要单独指定字体大小</p><h3 id="font-weight-设置字体粗细-font-weight-normal-bold-bolder-lighter-number"><a href="#font-weight-设置字体粗细-font-weight-normal-bold-bolder-lighter-number" class="headerlink" title="font-weight 设置字体粗细 font-weight: normal|bold|bolder|lighter|number;"></a>font-weight 设置字体粗细 font-weight: normal|bold|bolder|lighter|number;</h3><p>当属性值为number时，直接写数字，数字为100、200、…900,一定不要加单位px，其中400就是对应normal</p><h3 id="font-style设置字体风格（是否是斜体等）"><a href="#font-style设置字体风格（是否是斜体等）" class="headerlink" title="font-style设置字体风格（是否是斜体等）"></a>font-style设置字体风格（是否是斜体等）</h3><p><code>font-style: normal|italic;</code></p><h3 id="font复合属性写法-（相当于对font的多个属性做一个简洁的写法）"><a href="#font复合属性写法-（相当于对font的多个属性做一个简洁的写法）" class="headerlink" title="font复合属性写法 （相当于对font的多个属性做一个简洁的写法）"></a>font复合属性写法 （相当于对font的多个属性做一个简洁的写法）</h3><p>要求格式为font: font-style font-weight font-size/line-height font-family;<br>其中line-height可以是固定的多少px，也可以没有单位如1.5，意思是行高是font-size的1.5倍，可以根据自己的文字大小自动调整行高（在CSS的继承性中要注意）</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">font: italic bold 20px &quot;微软雅黑&quot;;</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">* 以上属性不可以颠倒顺序</span><br><span class="line">* 以上属性不需要的可以省略，但必须有font-size和font-family,否则font属性不起作用</span><br><span class="line"></span><br><span class="line">## CSS文本属性  </span><br><span class="line">CSS Text（文本）属性可以定义文本的外观，如文本颜色、对齐文本、装饰文本、文本缩进、行间距等  </span><br><span class="line"></span><br><span class="line">### color设置文本颜色 ```color: red;```</span><br><span class="line"></span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;表示&lt;/th&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;预定义的颜色值&lt;/td&gt;&lt;td&gt;red green blue pink&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;十六进制&lt;/td&gt;&lt;td&gt;#FF0000 #FF6600&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;RGB代码&lt;/td&gt;&lt;td&gt;rgb(255,0,0)或者rgb(100%,0%,0%)&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line">### text-align设置文本水平对齐方式 text-align: center|left|right;  </span><br><span class="line">默认左对齐</span><br><span class="line">### text-decoration属性添加文本的修饰，可以给文本添加上划线、下划线、删除线等  </span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;th&gt;描述&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;none&lt;/td&gt;&lt;td&gt;默认，没有装饰线&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;underline&lt;/td&gt;&lt;td&gt;下划线（链接a自带下划线）&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;overline&lt;/td&gt;&lt;td&gt;上划线&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;line-through&lt;/td&gt;&lt;td&gt;删除线&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line">### text-indent属性用来指定文本的第一行缩进，通常是将段落的首行缩进 text-indent：20px;</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;th&gt;含义&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;20px&lt;/td&gt;&lt;td&gt;缩进20个像素&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;2em&lt;/td&gt;&lt;td&gt;缩进2个文字的大小.（em）是一个相对单位，&lt;br&gt;就是当前元素1个文字的大小（font-size）&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt; </span><br><span class="line"></span><br><span class="line">### line-height属性用来定义行间距（行高） line-height: 26px;  </span><br><span class="line">行间距包含了上间距、文本高度和下间距  </span><br><span class="line">### 单行文字垂直居中</span><br><span class="line">CSS没有提供直接垂直居中的代码，我们可以使用一个小技巧实现单行文字的垂直居中 =&gt; 解决方案：让文字的行高(line-height)等于盒子的高度，就可以让文字在盒子内垂直居中 </span><br><span class="line"># CSS的引入方式  </span><br><span class="line">根据CSS书写的位置（引入的方式），CSS样式表可以分成三大类：内部样式表（嵌入式）、行内样式表（行内式）、外部样式表（链接式）</span><br><span class="line">## 内部样式表  </span><br><span class="line"></span><br><span class="line">* 内部样式表（嵌入样式表）是写到html页面内部、将所有的CSS代码抽取出来，统一放到一个&lt; style&gt;标签对中  </span><br><span class="line">* 理论上&lt; style&gt;标签可以放到&lt; html&gt;标签中的任何一个地方，但是一般放在&lt; head&gt;标签中  </span><br><span class="line">* 通过内部样式表，可以方便控制当前整个页面中的元素样式设置  </span><br><span class="line">* 代码结构清晰，但是没有真正实现结构与样式的分离  </span><br><span class="line"></span><br><span class="line">## 行内样式表</span><br><span class="line"></span><br><span class="line">* 行内样式表（内联样式表）是指在html标签中通过指定style属性设置css样式，适用于修改简单样式  </span><br><span class="line">* style其实就是标签的属性  </span><br><span class="line">* 在双引号中间，写法要符合CSS规范  </span><br><span class="line">* 可以控制当前的标签设置样式  </span><br><span class="line">* 结构和样式不分离</span><br><span class="line"></span><br></pre></td></tr></table></figure><div style= "color: red;text-align: center;">啦啦啦</div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 外部样式表</span><br><span class="line"></span><br><span class="line">* 实际开发中都是使用外部样式表的形式，将样式单独写到CSS文件中，适用于样式比较多的情况，之后再引入到html文件中  </span><br><span class="line">* 外部样式表的使用步骤：①写一个.css文件，在这个文件中只写样式，不写&lt; style&gt;标签；②在html页面中使用&lt; link&gt; 标签引入这个css文件  </span><br><span class="line">* 同一个样式表可以控制多个html页面  </span><br><span class="line">* 实现结构和页面的完全分离</span><br><span class="line"></span><br></pre></td></tr></table></figure><html>    <head>        ...        <link rel="stylesheet" href="css路径">    </head>    <body>        ...    </body></html><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性&lt;/th&gt;&lt;th&gt;含义&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;rel&lt;/td&gt;&lt;td&gt;定义当前文档与被链接文档之间的关系，这里指定为&quot;stylesheet&quot;，&lt;br&gt;表示被链接文档为一个样式表&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;href&lt;/td&gt;&lt;td&gt;被链接文档的路径&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line"># Emmet语法  </span><br><span class="line">Emmet语法的前身是Zen coding，它使用缩写，来提高html/css的编写速度Vscode内部已经集成该语法 </span><br><span class="line"></span><br><span class="line">## 快速生成HTML结构语法  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1. 生成标签：输入标签名直接按tab键即可</span><br><span class="line">2. 如果想要生成多个标签，则在后边添加*+数字再按tab键即可，如div*3就可以快速生成3个div  </span><br><span class="line">3. 父子级关系的标签用&gt;,如ul&gt;li再tab键  </span><br><span class="line">4. 兄弟关系的标签用+,如div+p再tab键</span><br><span class="line">5. .+类名再按tab键 得到具有class属性的标签（默认是div标签，如果是想要其他标签的话，就是 标签名+.+类名，如p.one和&lt; p class=&quot;one&quot;&gt;&lt; /p&gt;效果是一样的）</span><br><span class="line">6.  #+id名再按tab键 得到具有id属性的标签（默认是div标签，如果是想要其他标签的话，就是 标签名+#+id名，如p#one和&lt; p id=&quot;one&quot;&gt;&lt; /p&gt;效果是一样的） </span><br><span class="line">7. 标签名+./#+名字+$+*数字  再按tab键 就是具有class或者id属性并且属性值按照增量自增加的数字个标签  </span><br><span class="line">8. 如果想要在生成的标签内部写内容可以用&#123;&#125;表示</span><br><span class="line"></span><br><span class="line">## 快速生成CSS样式语法  </span><br><span class="line">CSS采取简写形式即可，如：  </span><br><span class="line">1. w200 按tab 可以生成width:200px;</span><br><span class="line">2. lh26 按tab 可以生成line-height:26px;  </span><br><span class="line"></span><br><span class="line"># CSS的元素显示模式</span><br><span class="line">元素显示模式就是元素（标签）以什么方式进行显示，比如&lt; div&gt;自己占一行，而&lt; span&gt;一行可以放多个  </span><br><span class="line">HTML元素一般分为块元素和行内元素两种</span><br><span class="line">## 块元素</span><br><span class="line">常见的块元素有&lt; h1&gt; ~ &lt; h6&gt; &lt; p&gt; &lt; div&gt; &lt; ul&gt; &lt; ol&gt; &lt; li&gt;  </span><br><span class="line">块元素的特点：</span><br><span class="line">* 独占一行</span><br><span class="line">* 高度、宽度、内外边距都可以自己控制</span><br><span class="line">* 宽度默认是容器（父级元素宽度）的100%</span><br><span class="line">* 是一个容器及盒子，里边可以放块元素或者行内元素  </span><br><span class="line">块元素的注意事项：</span><br><span class="line">* 文字类标签如&lt; p&gt; 或者&lt; h2&gt;内部不能使用块标签</span><br><span class="line"></span><br><span class="line">## 行内元素（内联元素）</span><br><span class="line">&lt; span&gt;是典型的行内元素  </span><br><span class="line">行内元素的特点：</span><br><span class="line">* 一行内可以有多个行内元素</span><br><span class="line">* 行内元素无法指定宽和高，默认宽高就是内容的宽高</span><br><span class="line">* 行内元素只能容纳文本或者其他行内元素  </span><br><span class="line">行内元素注意事项：</span><br><span class="line">* 链接里边不能再放链接</span><br><span class="line">* 特殊情况链接标签&lt; a&gt;里边可以放块级元素，但是给&lt; a&gt;转换一下块级模式更安全  </span><br><span class="line"></span><br><span class="line">## 行内块元素  </span><br><span class="line">在行内元素中有几个特殊标签——&lt; img&gt; &lt; input&gt; &lt; td&gt;标签兼具行内元素和块元素的特点，因此有时被称为行内块元素  </span><br><span class="line">行内块元素特点：</span><br><span class="line">* 一行可以有多个行内块元素（行内元素特点）</span><br><span class="line">* 默认宽度为本身内容宽度（行内元素特点）</span><br><span class="line">* 可以对元素的宽度、行高、内外边距进行设置（块元素特点）</span><br><span class="line"></span><br><span class="line">## 元素显示模式转换</span><br><span class="line">有时候我们需要对元素的显示模式进行转换，简单来说就是一个模式的元素需要另一种模式的特性,比如需要增加链接标签&lt; a&gt; 的触发范围  </span><br><span class="line">* 转换为块级元素  display: block;</span><br><span class="line">* 转换成行内元素  display: inline;</span><br><span class="line">* 转换为行内块元素  display: inline-block;</span><br><span class="line"></span><br></pre></td></tr></table></figure><head>    ...    <style>        a {             width；150px;            height: 50px;            background-color: red;            display: block;          }<pre><code>    div &#123;         display: inline;        &#125;&lt;/style&gt;</code></pre></head><body>    ...    <a href = "#"> 我是行内元素 </a>    <div>我是块元素</div>    ...</body><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 单行文字垂直居中的实现和原理</span><br><span class="line">* 实现：文字的行高等于容器的高度</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>div {    ...    height:40px;    ...    line-height:40px;}<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* 原理：文字的行高包括了上空隙、文字本身高度以及下空隙，上述方法简单理解就是行高的上空隙和下空隙把文字挤到中间了（上空隙和下空隙是相等的）。如果行高小于盒子高度，文字会偏上；如果行高大于盒子的高度，文字会偏下  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># CSS的背景</span><br><span class="line">通过CSS背景属性，可以给页面元素添加背景样式，背景属性可以设置背景颜色、背景图片、背景平铺、背景图片位置、背景图像固定等</span><br><span class="line">## 背景颜色</span><br><span class="line">background-color属性定义了元素的背景颜色</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>background-color:red;<br>/* 一般情况下背景颜色默认是transparent（透明），我们也可以手动指定*/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 背景图片</span><br><span class="line">background-image属性描述了元素的背景图像，实际开发常见于logo或者是一些装饰性的小图片或者是一些超大的背景图片，优点是非常便于控制位置（精灵图也是一种应用）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>background-image: url(images/logo.png)<br>/* 一定不要忘记写url()*/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 背景平铺</span><br><span class="line">如果要在HTML页面上对背景图片进行平铺，可以使用background-repeat属性，该属性的取值为 repeat(默认)|no-repeat|repeat-x|repeat-y </span><br><span class="line"></span><br><span class="line">## 背景图片位置</span><br><span class="line">可以通过background-position属性改变图片在背景中的位置  </span><br><span class="line">background-position: x y;  </span><br><span class="line">参数代表的是x坐标和y坐标，可以使用&lt;b&gt;方位名词&lt;/b&gt;或者&lt;b&gt;精确单位&lt;/b&gt;</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;th&gt;参数值&lt;/th&gt;&lt;th&gt;说明&lt;/th&gt;</span><br><span class="line">&lt;tr&gt;</span><br><span class="line">&lt;td&gt;length&lt;/td&gt;&lt;td&gt;百分数|由浮点数字和单位标识符组成的长度值&lt;/td&gt;</span><br><span class="line">&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;</span><br><span class="line">&lt;td&gt;position&lt;/td&gt;&lt;td&gt;top|center|bottom|left|center|right&lt;/td&gt;</span><br><span class="line">&lt;/tr&gt;</span><br><span class="line"></span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line">* 方位名词不分先后：right center和center right效果是一样的</span><br><span class="line">* 如果指定了一个方位名词，另一个值省略，则第二个省略的值默认居中</span><br><span class="line">* 参数是精确单位时，第一个肯定是x坐标，第二个肯定是y坐标</span><br><span class="line">* 精确单位如果只指定了一个值，则该值一定是x坐标，另一个坐标默认垂直居中</span><br><span class="line">* 如果指定的两个值是精确单位和方位名词的混合使用，则第一个值为x坐标，第二个值为y坐标</span><br><span class="line"></span><br><span class="line">## 背景图像固定（背景附着）  </span><br><span class="line">background-attachment属性设置背景图像是否固定或者随着页面的其他部分滚动，后期可以制作视差滚动的效果  </span><br><span class="line">该属性的属性值为scroll(背景图像随对象内容滚动)或者fixed（背景图像固定）</span><br><span class="line"></span><br><span class="line">## 背景复合写法</span><br><span class="line">为了简化背景属性的代码量，我们可以将这些属性合并简写在同一个属性background中  </span><br><span class="line">当使用简写属性时，没有特定的书写顺序，一般习惯的书写顺序为：  </span><br><span class="line">background: 背景颜色 背景图片地址 背景平铺 背景图像滚动 背景图片位置</span><br><span class="line"></span><br><span class="line">## 背景色半透明</span><br><span class="line">background: rgba(0,0,0,0.3)  </span><br><span class="line">最后一个参数是alpha透明度，取值范围在0~1之间</span><br><span class="line"></span><br><span class="line"># CSS的三大特性</span><br><span class="line">CSS有三个非常重要的特性：层叠性、继承性、优先级</span><br><span class="line">## 层叠性</span><br><span class="line">当给相同选择器设置相同的样式时，此时的样式会&lt;b&gt;覆盖（层叠）&lt;/b&gt;另一个冲突的样式，层叠性主要解决样式冲突的问题  </span><br><span class="line">层叠性原则：</span><br><span class="line">* 样式冲突时遵循&lt;b&gt;就近原则&lt;/b&gt;,哪个样式离结构近，就执行哪个样式</span><br><span class="line"></span><br><span class="line">## 继承性</span><br><span class="line">子标签会继承父标签的某些样式，如文本颜色和字号(text-,font-,line-这些元素开头的可以继承，以及color属性)</span><br><span class="line"></span><br><span class="line">## 优先级</span><br><span class="line">当一个元素指定多个选择器，就会有优先级的产生</span><br><span class="line">* 选择器相同，则执行层叠性</span><br><span class="line">* 选择器不同，则根据选择器权重执行  </span><br><span class="line">选择器权重如下表所示</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;选择器&lt;/th&gt;&lt;th&gt;选择器权重&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;继承或者*（通配）&lt;/td&gt;&lt;td&gt;0，0，0，0&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;元素（标签）选择器&lt;/td&gt;&lt;td&gt;0，0，0，1&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;类选择器&lt;/td&gt;&lt;td&gt;0，0，1，0&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;ID选择器&lt;/td&gt;&lt;td&gt;0，1，0，0&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;行内样式 style=&quot;&quot;&lt;/td&gt;&lt;td&gt;1，0，0，0&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;！important(写在某个选择器的样式属性之后)&lt;/td&gt;&lt;td&gt;∞ 无穷大&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line">注：a链接在默认情况下由浏览器指定了一个蓝色的有下划线的样式（不会继承父类的样式，因为继承的权重是0）</span><br><span class="line"></span><br><span class="line">* 复合选择器会有权重叠加的问题，但是要注意权重有叠加但永远不会有进位问题</span><br><span class="line"># CSS盒子模型</span><br><span class="line">页面布局要学习三大核心，盒子模型、浮动和定位，学习好盒子模型能帮助我们布局页面  </span><br><span class="line">CSS盒子模型本质上是一个盒子，封装周围的HTML元素，它包括：边框、外边距、内边距和实际内容  </span><br><span class="line">border边框  content内容 padding内边距 外边距margin</span><br><span class="line">## 边框(border)</span><br><span class="line">border可以设置边框的属性，包括边框宽度（粗细）、边框样式（实线/虚线）、边框颜色  </span><br><span class="line">* border-width:定义边框粗细，单位是px；</span><br><span class="line">* border-style:边框样式，none(默认)|hidden|dotted|dashed|solid|double|groove|ridge|inset|outset</span><br><span class="line">* border-color:边框颜色</span><br><span class="line">* 边框的复合写法（没有顺序）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>border:1px solid red;</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* 盒子有4条边，可以分开写:border-top,border-bottom,border-right,border-left</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>border-top: 1px solid red;</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* border-collapse:当该属性值为collapse时表示合并相邻的边框</span><br><span class="line">* 边框会影响盒子的大小（会变大）</span><br><span class="line">## 内边距(padding)</span><br><span class="line"></span><br><span class="line">* 同样有4个:padding-top/bottom/left/right,单位是px</span><br><span class="line">* padding的复合属性</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;padding:5px;&lt;/td&gt;&lt;td&gt;上下左右都是5px&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;padding:5px 10px;&lt;/td&gt;&lt;td&gt;上下5px左右10px&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;padding:5px 10px 15px;&lt;/td&gt;&lt;td&gt;上5左右10下15&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;padding:5px 10px 15px 20px&lt;/td&gt;&lt;td&gt;上右下左分别是5、10、15、20px（顺时针）&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line">* padding会影响盒子的实际大小（也是变大，撑大盒子）</span><br><span class="line">* 当遇到盒子内文字数量不同时可以用padding撑开</span><br><span class="line">* 如果盒子本身没有写height/width属性（继承自其父亲），则padding不会撑开盒子</span><br><span class="line">## 盒子模型外边距(margin)</span><br><span class="line">* 使用方法类似于padding：4个，复合属性（margin好像并不会像padding一样撑大盒子，也就是不会影响盒子的实际大小）</span><br><span class="line">* 外边距典型应用：块级盒子水平居中  </span><br><span class="line">外边距可以让盒子水平居中，但必须满足两个条件：  </span><br><span class="line">① 盒子必须指定宽度(width)  </span><br><span class="line">② 盒子左右的外边距设置为auto  </span><br><span class="line">常见的写法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>margin-left:auto;margin-right:auto;<br>margin:auto;<br>margin:0 auto;</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">以上方法是让块元素水平居中，行内元素或者行内块元素水平居中的话给其&lt;b&gt;父元素&lt;/b&gt;添加&lt;b&gt;text-align:center&lt;/b&gt;即可</span><br><span class="line">* 外边距合并：上下两个相邻元素分别同时有margin-bottom和margin-top时，它们之间的外边距并不是两个外边距的和，而是两个外边距的较大的值，这种现象称为&lt;b&gt;相邻块元素垂直外边距的合并&lt;/b&gt;，解决方法就是尽量只给一个盒子添加margin值  </span><br><span class="line">* 嵌套块元素垂直外边距的塌陷：对于两个存在嵌套关系（父子关系）的元素来说，如果在父元素有上边距的同时给子元素加上上边距，则父元素会塌陷较大的外边距值  </span><br><span class="line">解决方案：  </span><br><span class="line">① 给父元素添加border-top</span><br><span class="line">② 给父元素添加padding-top</span><br><span class="line">③ 为父元素添加overflow:hidden  </span><br><span class="line">&lt;b&gt;&lt;i&gt;但是要注意，针对以上两个问题，有如下的特殊情况：两个相邻元素都有外边距，只要有一个浮动，那么就不会出现外边距合并；父子盒子只要有一个盒子浮动，就不会出现塌陷&lt;/i&gt;&lt;/b&gt;</span><br><span class="line"></span><br><span class="line">* 清除内外边距：不同的网页元素可能自身带有特定的内外边距，导致我们直接写代码的话产生的效果不是我们理想的，可以采用以下写法进行清除（* 代表通配符选择器）</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>{<br>  padding: 0;<br>  margin: 0;<br>}<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* 行内元素为了照顾兼容性，尽量只设置左右的内外边距（设置上下的并不会起作用），但是转换为块级和行内块元素就可以了</span><br><span class="line">## CSS3新添加的属性</span><br><span class="line">### 圆角边框：盒子变成圆角</span><br><span class="line">圆和矩形拼接（椭圆和边框的交集形成圆角）</span><br><span class="line"></span><br></pre></td></tr></table></figure>border-radius: length(该属性值可以是数值（单位是px），或者是百分比（当盒子为正方形时相当于radius是width/height的百分比倍）);<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* 该属性是一个简写属性，后边可以跟4个值，分别代表左上、右上、右下、左下角（当然也可以跟2个或者3个值，代表不同的含义）</span><br><span class="line">* 这个属性也可以分开去写，如border-top-left-radius等，以此类推</span><br><span class="line"></span><br><span class="line">### 盒子阴影</span><br><span class="line">使用box-shadow添加盒子阴影效果  </span><br><span class="line">box-shadow: &lt;b&gt;h-shadow&lt;/b&gt; &lt;b&gt;v-shadow&lt;/b&gt; blur spread color inset;</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;值&lt;/th&gt;&lt;th&gt;描述&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;h-shadow&lt;/td&gt;&lt;td&gt;必需，水平阴影的位置，允许负值&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;v-shadow&lt;/td&gt;&lt;td&gt;必需，垂直阴影的位置，允许负值&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;blur&lt;/td&gt;&lt;td&gt;可选，模糊距离&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;spread&lt;/td&gt;&lt;td&gt;可选，阴影的尺寸&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;color&lt;/td&gt;&lt;td&gt;可选，阴影的颜色&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;inset&lt;/td&gt;&lt;td&gt;可选，将外部阴影改为内部阴影&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line">注：  </span><br><span class="line"></span><br><span class="line">* 默认是外阴影，但不能写这个单词(outset)，否则样式会失效</span><br><span class="line">* 盒子阴影不占用空间，不会影响其他盒子</span><br><span class="line"></span><br><span class="line">### 文字阴影  </span><br><span class="line">使用text-shadow添加文字阴影(类似于box-shadow)  </span><br><span class="line">text-shadow: &lt;b&gt;h-shadow&lt;/b&gt; &lt;b&gt;v-shadow&lt;/b&gt; blur color;</span><br><span class="line"></span><br><span class="line"># CSS浮动</span><br><span class="line">## 浮动概览</span><br><span class="line">### 传统网页布局的三种方式  </span><br><span class="line">网页布局的本质——用CSS摆放盒子，把盒子摆放到相应的位置  </span><br><span class="line">CSS提供了3种传统的网页布局方式，简单来说，就是盒子如何摆放：</span><br><span class="line">* 普通流（标准流/文档流）:标签按照默认规定好的方式排列</span><br><span class="line">* 浮动</span><br><span class="line">* 定位</span><br><span class="line">### 为什么需要浮动</span><br><span class="line">浮动可以改变标签默认的排列方式  </span><br><span class="line">网页布局第一准则：&lt;b&gt;多个块级元素纵向排列找标准流，多个块内元素横向排列找浮动&lt;/b&gt;</span><br><span class="line">### 什么是浮动</span><br><span class="line">* float属性用于创建浮动框，将其移动到一边，直到左边缘或者右边缘触及包含快或另一个浮动框的边缘  </span><br><span class="line">* 语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure>选择器{<br>  float: 属性值;<br>  }<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;th&gt;描述&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;none&lt;/td&gt;&lt;td&gt;元素不浮动（默认值）&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;left&lt;/td&gt;&lt;td&gt;元素向左浮动&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;right&lt;/td&gt;&lt;td&gt;元素向右浮动&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;  </span><br><span class="line"></span><br><span class="line">### 浮动特性</span><br><span class="line">1. 浮动元素会脱离标准流（脱标），浮动的元素不再保留原有的位置</span><br><span class="line">2. 浮动的元素会一行内显示并元素顶部对齐（装不下会另起一行）</span><br><span class="line">3. 浮动元素会有行内块元素的特性</span><br><span class="line">4. 浮动的盒子只会影响浮动盒子后边的标准流而不会影响前面的标准流（前面的标准流还是独占一行，接下来的浮动元素会把前边的标准流的底部作为自己能占据的顶部极限）</span><br><span class="line">### 浮动元素经常和标准流父级搭配使用</span><br><span class="line">为了约束浮动元素位置，网页布局一般采取的策略是：  </span><br><span class="line">&lt;b&gt;先用标准流的父元素排列上下位置，之后内部子元素采取浮动排列左右位置，符合网页布局第一准则&lt;/b&gt;  </span><br><span class="line">&lt;b&gt;网页布局第二准则：先设置盒子大小，之后设置盒子位置&lt;/b&gt;</span><br><span class="line"></span><br><span class="line">## 清除浮动 </span><br><span class="line">### 为什么需要清除浮动？</span><br><span class="line">有时候我们希望父元素的高度不是固定的，因为里边的内容数量和高度是不确定的，但是如果我们不指定父元素的高度的话，会出现一个问题，就是当父元素种装的是浮动元素时，父元素的高度会变成0，这是因为浮动元素是“不占位置”的</span><br><span class="line">### 清除浮动的本质</span><br><span class="line">* 清除浮动的本质就是清除浮动元素的影响</span><br><span class="line">* 如果父盒子本身有高度，则不需要清除浮动</span><br><span class="line">* 清除浮动之后，父级就会根据浮动的子盒子自动检测高度，父级有了高度，就不会影响下边的标准流了</span><br><span class="line">### 清除浮动的语法</span><br><span class="line">选择器 &#123;</span><br><span class="line">    clear: 属性值;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;th&gt;描述&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;left&lt;/td&gt;&lt;td&gt;清除左浮动影响&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;right&lt;/td&gt;&lt;td&gt;清除右浮动影响&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;both&lt;/td&gt;&lt;td&gt;影响都清除&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line">清除浮动的策略是：闭合浮动（孩子只在家里皮）  </span><br><span class="line"></span><br><span class="line">### 清除浮动的方法</span><br><span class="line">* 额外标签法也称隔墙法，是W3C推荐的做法  </span><br><span class="line">额外标签法会在浮动元素末尾添加一个空的标签，例如&lt; div style=&quot; clear:both&quot;&gt;&lt; /div&gt;或者其他标签（如&lt; br&gt;等），通俗易懂，但添加许多无意义标签导致结构性较差  </span><br><span class="line">注意：&lt;b&gt;要求新添加的标签必须是块级元素&lt;/b&gt;</span><br><span class="line">* 父级添加overflow属性  </span><br><span class="line">子不教，父之过，注意是给父级元素添加代码：给父级元素添加overflow属性，其属性值可以设置为hidden、auto或者scroll  </span><br><span class="line">前边讲的外边距合并的时候也是通过这个属性实现的  </span><br><span class="line">代码简洁但是无法显示溢出的部分</span><br><span class="line"></span><br><span class="line">* 父级添加after伪元素  </span><br><span class="line">:after是额外标签法的升级版，也是给父元素添加  </span><br><span class="line"></span><br></pre></td></tr></table></figure>.clearfix:after{<br>  content: “”;<br>  display: block;<br>  height: 0;<br>  clear: both;<br>  visibility: hidden;</li></ul><p>}<br>.clearfix{<br>    /<em>IE6、7专有</em>/<br>    *zoom: 1;<br>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">相当于在父元素后边插入了一个元素，但是其实并没有引入新的标签，但是缺点是需要兼容低版本浏览器</span><br><span class="line">* 父级添加双伪元素  </span><br><span class="line">相当于前后都添加了元素（堵住）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>.clearfix:before,.clearfix.after {<br>    content: “”;<br>    display: table;<br>}<br>.clearfix:after {<br>    clear: both;<br>}<br>.clearfix {<br>    *zoom: 1;<br>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">优缺点同添加after伪元素</span><br><span class="line"></span><br><span class="line"># 定位</span><br><span class="line">## 定位概述</span><br><span class="line">### 为什么需要定位</span><br><span class="line">1. 使某个元素可以在盒子内自由移动，并且压住其他盒子</span><br><span class="line">2. 当我们滚动窗口时，盒子是固定在屏幕某个位置的</span><br><span class="line">### 定位组成</span><br><span class="line">定位其实就是按照定位的方式摆盒子，定位=定位模式+边偏移</span><br><span class="line">1. 定位模式：定位模式决定元素的定位方式，它通过CSS的position属性进行设置，属性值如下：</span><br><span class="line">&lt;table&gt;</span><br><span class="line">&lt;tr&gt;&lt;th&gt;属性值&lt;/th&gt;&lt;th&gt;含义&lt;/th&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;static&lt;/td&gt;&lt;td&gt;静态定位&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;relative&lt;/td&gt;&lt;td&gt;相对定位&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;absolute&lt;/td&gt;&lt;td&gt;绝对定位&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;tr&gt;&lt;td&gt;fixed&lt;/td&gt;&lt;td&gt;固定定位&lt;/td&gt;&lt;/tr&gt;</span><br><span class="line">&lt;/table&gt;</span><br><span class="line"></span><br><span class="line">2.边偏移就是定位的盒子移动到最终位置（相对于父元素上下左右边线的距离），有top bottom left right 4个属性（属性值单位为px，可以是负值） </span><br><span class="line"></span><br><span class="line">## 具体定位方式</span><br><span class="line">### 静态定位static（了解）</span><br><span class="line">是元素的默认定位方式，无定位的意思   </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>选择器 {position: static;}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">按照标准流位置摆放，没有边偏移</span><br><span class="line">### 相对定位relative（重要）</span><br><span class="line">相对定位是元素在移动位置的时候相对于它原来的位置说的   </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>选择器 {position: relative;}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">注：</span><br><span class="line">1. 原来在标准流的位置继续占有   </span><br><span class="line">### 绝对定位absolute（重要）  </span><br><span class="line">绝对定位在移动时是相对于它的祖先元素来说的  </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>选择器 {<br>    position: absolute;<br>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">绝对定位的特点：  </span><br><span class="line">1. 如果没有祖先元素或者祖先元素没有定位，则以浏览器为准定位  </span><br><span class="line">2. 如果祖先元素有定位（相对、绝对、固定定位），则以&lt;b&gt;最近一级有定位祖先元素&lt;/b&gt;为参考点移动位置  </span><br><span class="line">3. 绝对定位不再占有原有位置（脱离标准流） =&gt; 轮播图实现  </span><br><span class="line">### 关于子绝父相</span><br><span class="line">所谓“子绝父相”就是说子级是绝对定位的话，父级要用相对定位  </span><br><span class="line">1. 子级绝对定位，不会占有位置，可以放到父盒子里边的任何一个地方，不会影响其他的兄弟盒子  </span><br><span class="line">2. 父盒子需要加定位限制子盒子在父盒子内显示，父元素不能加绝对定位，这样可以防止绝对定位因为不占有位置导致下边的元素升上来产生影响，只能使用相对定位保留位置  </span><br><span class="line">但是“子绝父相”不是永远不变的，如果父元素不需要占有位置，“子绝父绝”也会遇到</span><br><span class="line">### 固定定位</span><br><span class="line">固定定位是元素固定于浏览器可视区的位置，主要应用场景：可以在浏览器页面滚动时元素的位置不改变  </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>选择器 { position: fixed;}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">固定定位的特点：</span><br><span class="line">1. 以浏览器的可视窗口（与浏览器页面大小有关）为参照点移动元素，与父元素没有关系且不随滚动条滚动</span><br><span class="line">2. 固定定位不再占有原有的位置</span><br><span class="line">3. 固定定位也可以看成是一种特殊的绝对定位</span><br><span class="line">#### 固定定位小技巧：固定在版心右侧位置</span><br><span class="line">1. 让固定定位的盒子left: 50%，走到浏览器可视区（也可以看作版心）的一半位置</span><br><span class="line">2. 让固定定位的盒子 margin-left: 版心宽度的一般距离，多走版心宽度的一半位置</span><br><span class="line">### 粘性定位（了解）</span><br><span class="line">可以被认为是相对定位和固定定位的结合，sticky  </span><br><span class="line">粘性定位特点：</span><br><span class="line">1. 以浏览器的可视窗口为参照点移动元素（固定定位特点）  </span><br><span class="line">2. 粘性定位占有原先的位置(相对定位特点)</span><br><span class="line">3. 必须添加top left right bottom这几个边偏移中的其中一个才有效  </span><br><span class="line">4. 和页面滚动搭配使用，但是兼容性较差，IE不支持</span><br><span class="line">## 定位叠放次序 z-index</span><br><span class="line">在使用定位布局时，可能会出现盒子重叠的情况，此时可以使用z-index来控制盒子的前后次序（z轴）  </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>选择器 { z-index: 1;}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* 属性值可以是正整数、负整数或者0，数值越大，盒子越靠上  </span><br><span class="line">* 如果属性值相同，则按照书写顺序，后来者居上  </span><br><span class="line">* 只有定位的盒子才有z-index属性</span><br><span class="line">## 定位的拓展</span><br><span class="line">### 绝对定位的盒子居中</span><br><span class="line">加了绝对定位的盒子是不能通过margin: 0 auto实现水平居中的，但是可以通过以下计算方法实现水平和垂直居中：  </span><br><span class="line">1. left: 50% 走到父元素宽度的50%</span><br><span class="line">2. margin-left: 负值 往左走自己盒子宽度的一半  </span><br><span class="line">垂直居中类似  </span><br><span class="line">&lt;b&gt;但是相对定位是可以通过margin: auto垂直居中的，因为相对定位是不脱离标准流的，和标准流一样&lt;/b&gt;</span><br><span class="line"></span><br><span class="line">### 定位特殊特性</span><br><span class="line">绝对定位和固定定位也和浮动类似  </span><br><span class="line">1. 行内元素添加绝对或者固定定位，可以直接设置高度和宽度  </span><br><span class="line">2. 块级元素添加绝对或者固定定位，如果不给宽度和高度，默认大小是内容大小  </span><br><span class="line">3. 脱标的盒子不会触发外边距塌陷：浮动元素、绝对定位（固定定位）元素都不会出发外边距合并问题</span><br><span class="line">4. 绝对定位（固定定位）会完全压住盒子  </span><br><span class="line">* 浮动元素不同，浮动元素只会压住元素下面标准流的盒子，但不会压住下面标准流盒子里边的文字（图片），之所以浮动不会压住文字，是因为浮动产生的目的最初是为了做文字环绕效果的，文字会围绕浮动元素    </span><br><span class="line">* 但是绝对定位（固定定位）会压住下面标准流所有的内容  </span><br><span class="line"></span><br><span class="line">## 关于元素的显示与隐藏</span><br><span class="line">类似于网站广告，点击关闭就不见了但是刷新页面会重新出现  </span><br><span class="line">本质：让一个元素在页面中隐藏或者显示出来（隐藏并非删除） </span><br><span class="line">### display 显示隐藏</span><br><span class="line">display属性用于设置元素如何显示</span><br><span class="line">1. display: none; 隐藏对象  </span><br><span class="line">2. display: block; 转换为块元素同时有显示元素的意思  </span><br><span class="line">display隐藏元素之后，不再占有原来的位置</span><br><span class="line">### visibility 显示隐藏</span><br><span class="line">visibility: inherit(继承上一个父对象的可见性)|visible(对象可视)|hidden(对象隐藏)|collapse(主要用来隐藏表格的行或者列)  </span><br><span class="line">visiblity隐藏元素之后，继续占有原来的位置</span><br><span class="line">### overflow 溢出显示隐藏 </span><br><span class="line">对溢出部分而非全部元素进行展示和隐藏  </span><br><span class="line">overflow: visible(默认显示超出部分内容)|auto(在需要时添加滚动条)|hidden(多出来的隐藏)|scroll(溢出部分显示滚动条，就算没有超出也有滚动条)  </span><br><span class="line">但是有定位的元素慎用overflow（有时是故意溢出的）</span><br><span class="line"># CSS高级知识</span><br><span class="line">## 精灵图</span><br><span class="line">### 为什么需要精灵图</span><br><span class="line">一个网页中往往会应用很多小的背景图像作为修饰，当网页中的图像过多时，服务器就会频繁地接收和发送请求图片，造成服务器请求压力过大，这将大大降低页面地加载速度  </span><br><span class="line">因此，为了有效减少服务器接收和发送请求地次数，提高页面地加载速度，出现了CSS精灵技术（也称CSS Sprites、CSS雪碧）  </span><br><span class="line">核心原理：讲网页中的一些小的背景图片整合到一张大图中，这样服务器只需要一次请求就可以了</span><br><span class="line">### 精灵图使用原理</span><br><span class="line">使用精灵图核心：  </span><br><span class="line">1. 精灵技术主要针对于背景图片使用，就是把多个小背景图片整合到一张大图片中  </span><br><span class="line">2. 这个大图片也称为sprites 精灵图 或者雪碧图  </span><br><span class="line">3. 移动背景图片位置，此时可以使用background-position  </span><br><span class="line">4. 移动的距离就是这个目标图片的x和y坐标，注意网页中的坐标有所不同（x越往右越大，y轴越往下越大）  </span><br><span class="line">5. 因为一般情况下都是往上往左移动，所以数值是负值  </span><br><span class="line">6. 使用精灵图时要精确测量，每个小背景图片的大小和位置  </span><br><span class="line">## 字体图标</span><br><span class="line">### 字体图标产生和优点  </span><br><span class="line">字体图标使用场景：主要用于显示网页中通用、常用的一些小图标  </span><br><span class="line">精灵图还是有很多优点的，但是缺点很明显：  </span><br><span class="line">* 图片文件比较大  </span><br><span class="line">* 图片本身放大和缩小会失真  </span><br><span class="line">* 一旦图片制作完成想要更改十分复杂  </span><br><span class="line">字体图标(iconfont)提供了一种方便高效的图标使用方式，展示的是图标，本质属于字体</span><br><span class="line">### 字体图标的使用</span><br><span class="line">下载 -&gt; 引入(在style标签中) -&gt; 使用 -&gt; 在style中给标签声明字体 font-family: &#x27;icomoon&#x27;; [-&gt; 追加]  </span><br><span class="line">因为本质是字体，所以字体颜色和大小都是可以改变的  </span><br><span class="line">## CSS三角做法  </span><br><span class="line">给一个没有宽度和高度的盒子添加四条有颜色的边框（得到长得像七巧板的图案）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>.box2 {<br>    width: 0;<br>    height: 0;<br>    border: 10px solid transparent;<br>    border-top-color: pink;<br>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 界面样式</span><br><span class="line">### 更改用户的鼠标样式 cursor</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>li {<br>    cursor: pointer;<br>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">default(小白，默认)  pointer(小手)  move(移动)  text(文本)  not-allowed（禁止）  </span><br><span class="line">### 取消表单轮廓</span><br><span class="line">给表单/文本域添加outline: 0;或者outline: none;样式之哦胡，就可以去掉默认的蓝色边框</span><br><span class="line">### 防止拖拽文本域</span><br><span class="line">对textarea标签添加resize: none;</span><br><span class="line">### vertical-align属性应用</span><br><span class="line">CSS的vertical-align属性使用场景：经常用于设置图片或者表单（行内块元素）和文字垂直对齐  </span><br><span class="line">用于设置一个元素的垂直对齐方式，但是只针对行内元素或者行内块元素有效  </span><br><span class="line">语法：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>vertical-align: baseline(默认，元素放置在父元素基线上)|top(元素与字体的顶部对齐)|middle(把此元素放在父元素中部)|bottom(把元素与字体底线对齐)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">以上样式是添加到图片或者表单中的 </span><br><span class="line">### 图片底侧空白缝隙解决  </span><br><span class="line">原因是图片默认是与文字的基线对齐，下边的空隙是留给文字基线下的内容的  </span><br><span class="line">主要解决方案：  </span><br><span class="line">1. 给图片添加vertical-align: middle|top|bottom;(只要不是基线对齐就行)  </span><br><span class="line">2. 把图片转换成块级元素（块级元素没有vertical-align属性，不会有基线对齐的方式） display: block;  </span><br><span class="line">### 溢出的文字省略号显示</span><br><span class="line">1. 单行  </span><br><span class="line">必须满足3个条件：  </span><br><span class="line">* 强制在一行内显示  white-space: nowrap;(默认normal会自动换行)  </span><br><span class="line">* 超出部分隐藏  overflow； hidden;  </span><br><span class="line">* 文字用省略号代替溢出部分  text-overflow: ellipsis;</span><br><span class="line">2. 多行(有较大兼容性问题,了解)  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>overflow: hidden;<br>text-overflow: ellipsis;<br>/<em>弹性伸缩盒子模型展示</em>/<br>display: -webkit-box;<br>/<em>限制在一个块元素显示的文本的行数</em>/<br>-webkit-line-clamp: 2;<br>/<em>设置或者检索伸缩盒对象的子元素的排列方式</em>/<br>-webkit-box-orient: vertical;</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## 常见布局技巧</span><br><span class="line">### margin负值的运用</span><br><span class="line">### 文字围绕浮动元素</span><br><span class="line">### 行内块的巧妙运用</span><br><span class="line">### CSS三角强化</span><br><span class="line">## CSS初始化</span><br><span class="line">所有页面都要先经过CSS初始化(CSS reset),消除不同浏览器对HTML文本呈现的差异性  </span><br><span class="line"># HTML5 CSS3提高部分</span><br><span class="line">## HTML5的新特性（都有兼容性问题）</span><br><span class="line">## 新增语义化标签</span><br><span class="line">## 新增多媒体标签</span><br><span class="line">### 视频：&lt; video&gt;</span><br><span class="line">只支持3种文件类型，最好使用.mp4类型  </span><br><span class="line">### 音频：&lt; audio&gt;</span><br><span class="line">只支持3种类型，最好用.mp3</span><br><span class="line">## HTML5新增input表单  </span><br><span class="line">type =很多东西</span><br><span class="line">## HTML5新增表单属性</span><br><span class="line">## CSS3的新特性</span><br><span class="line">### 新增选择器</span><br><span class="line">#### 属性选择器  </span><br><span class="line">匹配有某属性/某属性值为某特定值/某属性值以某特定值开头/某属性值以某特定值结尾/某属性值含有某特定值</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>input[属性]<br>input[属性=”值”]<br>input[属性^=”值”]<br>input[属性$=”值”]<br>input[属性*=”值”]</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">类选择器、属性选择器、伪类选择器的权重都是10(但是要记住[]才是属性选择器，前边的是标签或者id选择器什么的，上边看到的就是一个复合选择器，算权重的时候要注意)</span><br><span class="line">#### 结构伪类选择器</span><br><span class="line">结构伪类选择器主要根据文档结构来选择元素，而且用冒号（伪类选择器）来进行选择，常用于根据父级选择里边的子元素</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>E:first-child    /<em>匹配父元素中第一个子元素E</em>/<br>E:last-child    /<em>匹配父元素中的最后一个子元素E</em>/<br>E:nth-child(n)  /<em>匹配父元素中第n个子元素E，n可以是数字（从1开始，表示第几个孩子）、关键字（even偶数，odd奇数）和公式（从0开始每次+1 往下计算，2n表示偶数孩子等价于even，2n+1表示奇数，5n表示5的倍数，n+5表示从第5个开始，-n+5表示前5个）</em>/<br>E:first-of-type  /<em>指定类型E的第一个</em>/<br>E:last-of-type  /<em>指定类型E的最后一个</em>/<br>E:nth-of-type(n)  /<em>指定类型E的第n个</em>/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">nth-child和nth-of-type的区别：  </span><br><span class="line">1. nth-child会把所有的孩子都排个序号，找到第n个孩子之后再去看前边的标签（有可能选出来的孩子和标签不对应就什么都没选出来）  </span><br><span class="line">2. nth-of-type会针对指定元素进行排列序号</span><br><span class="line">#### 伪元素选择器</span><br><span class="line">伪元素选择器可以帮助我们利用CSS创建新标签元素，而不需要HTML标签，从而简化HTML结构  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>element::before {}  /* 在元素内部的前面插入内容::前后不加空格*/<br>element::after {}   /<em>在元素内部的后面插入内容</em>/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">before和after创建一个元素，但是新创建的这个元素在文档树中是找不到的，所以称为伪元素  </span><br><span class="line">before 和after必须有content属性，为空就写&#x27;&#x27;  </span><br><span class="line">要注意是在元素&lt;b&gt;内部&lt;/b&gt;的前后插入元素  </span><br><span class="line">伪元素选择器和标签选择器一样，权重为0001</span><br><span class="line">### 盒子模型</span><br><span class="line">CSS3中可以通过box-sizing来指定盒模型，有2个值，即可指定为content-box、border-box,这样计算盒子大小的方式就发生了改变  </span><br><span class="line">可以分成两种情况：</span><br><span class="line">1. box-sizing: content-box 盒子大小为width+padding+border（以前默认的）  </span><br><span class="line">2. box-sizing: border-box 盒子最终大小为width（不用计算了）</span><br><span class="line">### CSS3 2D转换</span><br><span class="line">转换(transform)是CSS3中具有颠覆性的特征之一，可以实现元素的位移(translate,类似于定位)、旋转(rotate)、缩放(scale)等效果  </span><br><span class="line">* 位移 </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>transform: translate(x,y);<br>transform: translateX(x);<br>transform: translateY(y);<br>/<em>x和y的单位是px，指沿x和y轴走多少，注意坐标轴的特点</em>/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">改变元素位置可以使用的方法：定位  margin  translate  </span><br><span class="line">注：  </span><br><span class="line">1. translate的最大优点是不会影响到其他元素的位置  </span><br><span class="line">2. translate中的百分比单位是相对于自身元素的，如translate: (50%,50%)（可以和定位结合实现居中效果）  </span><br><span class="line">3. 对行内标签没有效果  </span><br><span class="line">* 旋转  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>transform: rotate(n);<br>/<em>n 的单位是deg,表示度数，正值顺时针旋转，负值逆时针，默认旋转中心是元素中心点</em>/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">注：  </span><br><span class="line">1. 旋转可以和盒子边框结合实现三角  </span><br><span class="line">2. 可以使用transform-origin: x y;设置旋转中心点（可以给x y设置像素或者方位名词如top bottom left right center）  </span><br><span class="line">3. 可以实现翻页效果  </span><br><span class="line">* 缩放 </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>transform: scale(x,y);<br>/* x和y都是整数，没有单位，宽放大x倍，高放大y倍 */<br>transform: scale(2);<br>/<em>宽度和高度同比例修改，只有一个参数表示同比例放大多少倍</em>/</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">注：  </span><br><span class="line">1. scale修改大小的优势：不会影响其他盒子，而且可以设置缩放的中心点 transform-origin:  </span><br><span class="line">2. 如果有阴影的话使用scale阴影也会变大，可能不太美观</span><br><span class="line">* 2D转换综合写法  </span><br><span class="line">1. 同时使用多个转换，其格式为  transform: translate() rotate() scale()  </span><br><span class="line">2. 其顺序会影响转换效果（先旋转会改变坐标轴方向）  </span><br><span class="line">3. 当我们同时有位移和其他属性的时候，记得要将位移放到最前</span><br><span class="line"></span><br><span class="line">### CSS3 动画</span><br><span class="line">动画(animation)可以通过设置多个节点来精确控制一个或一组动画，常用来实现复杂的动画效果  </span><br><span class="line">相较于过渡，动画可以实现更多变化、更多控制，连续自动播放等效果  </span><br><span class="line">动画分为两步：①定义动画；②使用动画  </span><br><span class="line">#### 动画的基本使用</span><br><span class="line">1. 用keyframes定义动画（类似定义类选择器）  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>@keyframes 动画名称{<br>    0% {<br>        width: 100px;<br>        transform: translateX(0px);<br>    }<br>    100% {<br>        width: 200px;<br>        transform: translateX(1000px);<br>    }</p><p>}</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2. 元素使用动画（在对应元素的样式中添加animation-name:动画名称; animation-duration:持续时间;）  </span><br><span class="line">#### 动画常用属性</span><br><span class="line">其中速度曲线属性中可以取steps()这个属性值，指的是整个动画分几步完成，一卡一卡的（可以实现打字机效果、奔跑的小熊）</span><br><span class="line">#### CSS动画简写</span><br><span class="line">animation:动画名称 持续时间 运动曲线 何时开始 播放次数 是否反方向 动画起始或者结束的状态;  </span><br><span class="line">同一元素可以添加多个动画，用逗号分隔</span><br><span class="line">### 3D转换</span><br><span class="line">在网页的三维坐标系中，z轴往外面是正值  </span><br><span class="line">#### 3D位移: translate3d(x,y,z)  </span><br><span class="line">就是在2D移动的基础上多了个可以移动的轴  </span><br><span class="line">* transform: translateX(100px);  </span><br><span class="line">* transform: translateY(100px);  </span><br><span class="line">* transform: translateZ(100px);这个的单位一般跟px，且要跟着透视使用  </span><br><span class="line">* transform: translate3d(x,y,z);  </span><br><span class="line">#### 3D旋转: rotate3D(x,y,z)</span><br><span class="line">3D旋转可以让元素在三维平面内沿着x轴、y轴、z轴或者自定义轴进行旋转(左手准则)  </span><br><span class="line">* 沿x轴正方向旋转45deg  transform: rotateX(45deg);  </span><br><span class="line">上下翻页效果（上单杠）  </span><br><span class="line">* 沿y轴正方向旋转45deg  transform: rotateY(45deg);  </span><br><span class="line">左右翻书页   </span><br><span class="line">* 沿z轴正方向旋转45deg  transform: rotateZ(45deg);  </span><br><span class="line">类似于2D旋转，上帝视角往下看  </span><br><span class="line">* 沿自定义轴旋转45deg  transform: rotate3d(x,y,z,deg)</span><br><span class="line">#### 透视: persitive</span><br><span class="line">透视是写在被观察元素的父盒子上的,透视（视距）越小，元素越大  </span><br><span class="line">persitive: 500px;</span><br><span class="line">#### 3D呈现 transform-style  </span><br><span class="line">控制子元素是否开启三维立体空间  </span><br><span class="line">* transform-style: flat（默认值，子元素不开启3d立体空间）  </span><br><span class="line">*  transform-style: preserved-3d; 子元素开启立体空间  </span><br><span class="line">* 代码写给父级，但是影响的是子盒子</span><br><span class="line">### 其他特性</span><br><span class="line">#### CSS图片模糊-CSS3滤镜filter  </span><br><span class="line">filter: 函数();  </span><br><span class="line">例如：</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>filter: blur(5px);</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#### 计算盒子宽度calc函数</span><br><span class="line">calc()这个CSS函数能使我们在声明CSS属性值时执行一些计算  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>width: calc(100%-30px);</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">括号里面可以用+-*/来进行计算</span><br><span class="line">#### CSS3过渡（重点）</span><br><span class="line">过渡(transition)是CSS3中具有颠覆性的特征之一，可以在不使用flash动画或者javascript的情况下，当元素从一种样式变换为另一种样式时为元素添加效果，经常和:hover一起搭配使用  </span><br><span class="line">语法：（谁做过渡给谁加）  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>transition: 要过渡的属性 花费时间 运动曲线 何时开始;</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1. 属性：想要变化的css属性，宽度高度 背景颜色 内外边距都可以，如果想要所有的属性都变化过渡，写一个all就可以  </span><br><span class="line">2. 花费时间：单位是秒  </span><br><span class="line">3. 运动曲线：默认是ease(可以省略)  </span><br><span class="line">4. 何时开始：单位是秒，可以设置延迟触发时间，默认是0秒，可以省略  </span><br><span class="line">5. 同时添加多个动画效果的话，在第一组（是一组4个属性）属性后边加逗号再写第二组属性</span><br><span class="line"></span><br><span class="line"># 补充知识</span><br><span class="line">## 去掉 li 前边的项目符号（小圆点）</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>li {<br>    list-style: none;<br>    }</p><pre><code>## CSS属性书写顺序建议遵循以下顺序：  1. 布局定位属性：display/position/float/clear/visibility/overflow  2. 自身属性： width/height/margin/padding/border/background  3. 文本属性： color/font/text-decoration/text-align/vertical-align/white-space/break-word4. 其他属性（CSS3）： content/cursor/border-radius/box-shadow/text-shadow/background:linear-gradient...  ## 导航栏注意点在实际开发中，我们不会直接用链接a而是用li包含链接(li+a)的做法，语义更清晰，一看就是有条理的列表型内容## 行内块元素之间默认会有空隙，有时候盒子的位置不理想可以考虑是否有这个原因  ## favicon图标制作并引入html文件  ## 网站TDK三大标签SEO优化SEO：对网站进行深度优化，实现搜索引擎优化  三大标签： title description keywords## 实现选项卡的布局要求  包括两个部分：tab-list和tab-content  点击tab-list实现不同tab-content的切换  ## 一般情况下，a如果要包含有宽度的盒子，那么a需要转换为块级元素</code></pre>]]></content>
      
      
      <categories>
          
          <category> 前端学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HTML-1</title>
      <link href="/2022/01/27/%E5%89%8D%E7%AB%AF-html/"/>
      <url>/2022/01/27/%E5%89%8D%E7%AB%AF-html/</url>
      
        <content type="html"><![CDATA[<p> HTML 学习教程（一）</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><pre><code>HTML（hyper text markdown language）被称为超文本标记语言，一种用来描述网页的语言，本篇教程主要对HTML的主要标签进行整理~</code></pre><h1 id="HTML基本标签"><a href="#HTML基本标签" class="headerlink" title="HTML基本标签"></a>HTML基本标签</h1><h2 id="HTML结构标签"><a href="#HTML结构标签" class="headerlink" title="HTML结构标签"></a>HTML结构标签</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">        &lt;title&gt; 第一个页面 &lt;/title&gt;</span><br><span class="line">    &lt;/head&gt;</span><br><span class="line">    &lt;body&gt; Hello World! &lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><h2 id="body之前"><a href="#body之前" class="headerlink" title="body之前"></a>body之前</h2><h3 id="lt-DOCTYPE-gt-标签：文档类型声明标签-并不属于HTML语言"><a href="#lt-DOCTYPE-gt-标签：文档类型声明标签-并不属于HTML语言" class="headerlink" title="&lt;!DOCTYPE&gt;标签：文档类型声明标签(并不属于HTML语言)"></a>&lt;!DOCTYPE&gt;标签：文档类型声明标签(并不属于HTML语言)</h3><p>作用就是告诉浏览器使用哪种HTML版本来显示网页</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;  &lt;!--告诉浏览器当前页面是采用html5版本来显示网页，这句必须在文件第一行--&gt;</span><br></pre></td></tr></table></figure><h3 id="lang语言种类"><a href="#lang语言种类" class="headerlink" title="lang语言种类"></a>lang语言种类</h3><p>定义当前文档显示的语言:en 英语，zh-CN中文，定义为中文的文档也可以写英文，定义为英文的文档也可以写中文</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;html lang = &quot;en&quot;&gt;</span><br></pre></td></tr></table></figure><h3 id="meta标签"><a href="#meta标签" class="headerlink" title="meta标签"></a>meta标签</h3><meta>标签必须位于<head>标签内部  ① 在<head>标签中，可以通过<meta>标签的charset属性规定HTML文档使用哪种字符集，避免乱码.charset常用的值GB2312、GBK、BIG5和UTF-8② <meta> 标签中的name属性③ <meta>标签中的http-equiv属性③ <meta>标签中的content属性④ <meta>标签中的scheme属性<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=&quot;UTF-8&quot;/&gt;</span><br><span class="line">&lt;/head&gt;</span><br></pre></td></tr></table></figure><h2 id="body中"><a href="#body中" class="headerlink" title="body中"></a>body中</h2><h3 id="标题标签-lt-h1-gt-lt-h6-gt-双标签"><a href="#标题标签-lt-h1-gt-lt-h6-gt-双标签" class="headerlink" title="标题标签 &lt; h1 &gt;-&lt; h6 &gt; 双标签"></a>标题标签 &lt; h1 &gt;-&lt; h6 &gt; 双标签</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;h1&gt; 一级标题 &lt;/h1&gt;</span><br></pre></td></tr></table></figure><h3 id="段落标签-lt-p-gt-双标签"><a href="#段落标签-lt-p-gt-双标签" class="headerlink" title="段落标签 &lt; p &gt; 双标签"></a>段落标签 &lt; p &gt; 双标签</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;p&gt;我是一个段落&lt;/p&gt;</span><br></pre></td></tr></table></figure><h3 id="换行标签-lt-br-gt-单标签"><a href="#换行标签-lt-br-gt-单标签" class="headerlink" title="换行标签 &lt; br/ &gt; 单标签"></a>换行标签 &lt; br/ &gt; 单标签</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;p&gt; balabalabala &lt;br/&gt; balabalabala</span><br><span class="line">&lt;/p&gt;</span><br></pre></td></tr></table></figure><h3 id="文本格式化标签"><a href="#文本格式化标签" class="headerlink" title="文本格式化标签"></a>文本格式化标签</h3><ul><li>加粗<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;strong&gt;&lt;/strong&gt;</span><br><span class="line">&lt;b&gt;&lt;/b&gt;</span><br></pre></td></tr></table></figure></li><li>倾斜<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;em&gt;&lt;/em&gt;</span><br><span class="line">&lt;i&gt;&lt;/i&gt;</span><br></pre></td></tr></table></figure></li><li>下划线<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;ins&gt;&lt;/ins&gt;</span><br><span class="line">&lt;u&gt;&lt;/u&gt;</span><br></pre></td></tr></table></figure></li><li>删除线<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;del&gt;&lt;/del&gt;</span><br><span class="line">&lt;s&gt;&lt;/s&gt;</span><br></pre></td></tr></table></figure><h3 id="lt-div-gt-标签和-lt-span-gt-标签"><a href="#lt-div-gt-标签和-lt-span-gt-标签" class="headerlink" title="&lt; div &gt;标签和&lt; span &gt;标签"></a>&lt; div &gt;标签和&lt; span &gt;标签</h3>&lt; div &gt;和&lt; span &gt;是没有语义的，它们是一个盒子，用来装内容完成页面布局</li><li>&lt; div &gt;标签 双标签  </li></ul><p>一行只能放一个&lt; div &gt;标签，大盒子</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div&gt; 独占一行 &lt;/div&gt; </span><br></pre></td></tr></table></figure><ul><li><p>&lt; span &gt;标签 双标签<br>一行可以放多个&lt; span &gt;标签，小盒子</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;span&gt; 一行可以放多个span &lt;/span&gt;</span><br></pre></td></tr></table></figure><h3 id="图像标签"><a href="#图像标签" class="headerlink" title="图像标签"></a>图像标签</h3></li><li><p>图像标签 &lt; img src=”img.jpg” alt=”替换文本” title=”提示” width =”500”/ &gt;<br>src是图像标签的必备属性，指明了图像的src路径<br>alt属性值是文本，图像的替换文字<br>title属性值是文本，图像的提示文本<br>width属性值是像素，图像宽度像素值<br>height属性值是像素，图像高度像素值<br>border属性值是像素，边框粗细<br>注：属性之间不分先后顺序，标签与属性、属性与属性之间以空格分隔；属性采用键值对的格式，即key=”value”的格式</p></li><li><p>路径<br>该部分省略</p><h3 id="超链接标签"><a href="#超链接标签" class="headerlink" title="超链接标签"></a>超链接标签</h3></li><li><p>链接语法格式 &lt; a href=”跳转目标” target=”目标窗口的弹出方式”&gt;文本或图像或音视频或表格 &lt;/ a&gt;<br>href是超链接标签的必备属性，指定链接目标的url地址<br>target用于指定链接页面的打开方式，_self为默认值，_blank为在新页面中打开</p></li><li><p>链接类型<br>外部链接 ：href参数值为”外部某个网页地址”<br>内部链接 ：href参数值为”内部页面地址”<br>空链接 ：href参数值为#<br>下载链接 ：href参数值为”.exe 或者 .zip等文件格式的文件地址”<br>锚点链接 ：href属性值指定为”#名字”的形式并目标位置标签添加id属性为”名字”</p>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;a href=&quot;#one&quot;&gt;第一季&lt;/a&gt;</span><br><span class="line">&lt;h2 id=&quot;one&quot;&gt;第一季介绍&lt;/h2&gt;</span><br></pre></td></tr></table></figure><h3 id="注释-lt-–-–-gt-和特殊字符"><a href="#注释-lt-–-–-gt-和特殊字符" class="headerlink" title="注释&lt; !–    – &gt;和特殊字符"></a>注释&lt; !–    – &gt;和特殊字符</h3><h3 id="表格标签"><a href="#表格标签" class="headerlink" title="表格标签"></a>表格标签</h3></li><li><p>表格的主要作用是用来展示数据  </p></li><li><p>表格的基本语法  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;table&gt;</span><br><span class="line">    ...</span><br><span class="line">    &lt;tr&gt;</span><br><span class="line">        ...</span><br><span class="line">        &lt;td&gt;单元格内的文字&lt;/td&gt;</span><br><span class="line">        ...</span><br><span class="line">    &lt;/tr&gt;</span><br><span class="line">    ...</span><br><span class="line">&lt;/table&gt;</span><br></pre></td></tr></table></figure></li><li><p>表头单元格<br>&lt; th &gt;中间内容居中加粗显示&lt; /th &gt;，放在&lt; tr&gt;标签对内部，一般是位于表格的第一行或者第一列</p></li><li><p>表格相关属性  </p></li><li><p>注：表格标签属性不常用，一般通过css进行设置，写在&lt; table&gt;标签中  </p><table><tr><th>属性名</th><th>属性值</th><th>作用</th></tr><tr><td>align</td><td>left（默认）,center,right</td><td>相对于周围元素的对齐方式</td></tr><tr><td>border</td><td>1或""（默认）</td><td>是否有边框（默认没有）</td></tr><tr><td>cellpadding</td><td>像素值，默认1像素</td><td>单元边缘与内容间的空白</td></tr><tr><td>cellspacing</td><td>像素值，默认2像素</td><td>单元格之间的空白</td></tr><tr><td>width</td><td>像素值或者百分比</td><td>表格宽度</td></tr><tr><td>height</td><td>像素值或者百分比</td><td>表格高度</td></tr></table>  </li><li><p>表格结构标签<br>有些表格较长，为了更好地表达表格语义，可以将表格分为表格头部 &lt; thead&gt;和表格主体 &lt; tbody&gt;两个部分 </p></li><li><p>合并单元格<br>跨行合并：rowspan=”合并单元格数”，目标单元格在最上一个单元格<br>跨列合并：colspan=”合并单元格数”，目标单元格在最左边一个单元格<br>相当于是给目标单元格加一个”属性”吧</p><h3 id="列表标签"><a href="#列表标签" class="headerlink" title="列表标签"></a>列表标签</h3></li><li><p>表格的主要作用是实现简洁有序的布局</p></li><li><p>无序列表</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">    ...</span><br><span class="line">    &lt;li&gt;列表项1&lt;/li&gt;</span><br><span class="line">    &lt;li&gt;列表项2&lt;/li&gt;</span><br><span class="line">    &lt;li&gt;列表项3&lt;/li&gt;</span><br><span class="line">    ...</span><br><span class="line">&lt;/ul&gt;</span><br></pre></td></tr></table></figure><p>带有自己的样式属性，但是一般在css中设置</p></li><li><p>有序列表</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;ol&gt;</span><br><span class="line">    &lt;li&gt;有序列表项1&lt;/li&gt;</span><br><span class="line">    &lt;li&gt;有序列表项2&lt;/li&gt;</span><br><span class="line">    &lt;li&gt;有序列表项3&lt;/li&gt;</span><br><span class="line">&lt;/ol&gt;</span><br></pre></td></tr></table></figure></li><li><p>自定义列表<br>“一个大哥带着几个小弟”  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;dl&gt;</span><br><span class="line">    &lt;dt&gt;名词1&lt;/dt&gt;</span><br><span class="line">    &lt;dd&gt;名词解释1&lt;/dd&gt;</span><br><span class="line">    &lt;dd&gt;名词解释2&lt;/dd&gt;</span><br><span class="line">&lt;/dl&gt;</span><br></pre></td></tr></table></figure><p>&lt; dl&gt;中&lt; dd&gt;和&lt; dt&gt;标签的个数是没有限制的，而且&lt; dt&gt;和&lt; dd&gt;是兄弟关系而非包含关系</p><h3 id="表单标签"><a href="#表单标签" class="headerlink" title="表单标签"></a>表单标签</h3></li><li><p>使用表单的目的是为了收集用户信息。在HTML中，一个完整的表单由表单域、表单控件（也称为表单元素）和提示信息组成</p></li><li><p>表单域：包含表单元素的区域,会把&lt; form&gt;&lt; /form&gt;中的表单元素信息提交给服务器  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;form action=&quot;url地址&quot; method=&quot;提交方式&quot; name=&quot;表单域名称&quot;&gt;</span><br><span class="line">各种表单元素控件</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure><table><tr><th>属性</th><th>属性值</th><th>作用</th></tr><tr><td>action</td><td>url地址</td><td>用于指定接收并处理表单数据的服务器程序的url地址</td></tr><tr><td>method</td><td>get或者post</td><td>指定表单数据的提交方式</td></tr><tr><td>name</td><td>名称</td><td>用于指定表单的名称，以区分同一页面的多个表单域</td></tr></table>  </li><li><p>表单控件（表单元素）：允许用户在表单中选择或者输入的内容控件  </p></li><li><p>表单控件分类：<br>  ① input输入表单元素：单标签，&lt; input type=”属性值”/&gt;， type属性设置不同的属性值来指定不同的控件类型<br>  type属性分类：</p>  <table>  <tr><th>属性值</th><th>描述</th></tr>  <tr><td>text</td><td>输入文本</td></th>  <tr><td>password</td><td>输入密码</td></th>  <tr><td>radio</td><td>单选按钮</td></th>  <tr><td>checkbox</td><td>复选框</td></th>  <tr><td>submit</td><td>提交按钮</td></th>  <tr><td>reset</td><td>重置按钮</td></th>  <tr><td>button</td><td>定义可点击按钮（通常和JS结合）</td></th>  <tr><td>file</td><td>文件上传</td></th>  <tr><td>hidden</td><td>定义隐藏的输入字段</td></th>  <tr><td>image</td><td>定义图像形式的提交按钮</td></th>  </table>    除type属性之外的其他属性：  <table>  <tr><th>属性</th><th>属性值</th><th>描述</th</tr>  <tr><td>name</td><td>用户自定义</td><td>对表单控件进行区分，单选按钮必须拥有相同的name才能实现多选一，复选框也必须有相同的名字</td</tr>  <tr><td>value</td><td>用户自定义</td><td>规定input元素的值（文本 框中可以看到初始效果）</td</tr>  <tr><td>checked</td><td>checked</td><td>规定该input元素首次加载时应当被选中</td</tr>  <tr><td>maxlength</td><td>整数</td><td>规定输入字段中字符的最大长度</td</tr>  </table>    name和value是每个表单控件都应该有的元素，以备后台人员使用：name是表单控件的名字，要求单选按钮和复选框都必须有相同的name    checked属性主要针对单选按钮和复选框，主要作用是一打开某页面就可以默认选中某元素    < label>标签本身不属于表单标签，但是常与表单元素搭配使用，为< input>元素定义标注（< label>标签用于绑定一个表单元素，当点击< label>标签内的文本时，浏览器就会自动将焦点（光标）转到或者选择对应的表单元素上，增加用户体验）  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;label for=&quot;sex&quot;&gt;男&lt;/label&gt;</span><br><span class="line">&lt;input type=&quot;radio&quot; name=&quot;male&quot; id=&quot;sex&quot;/&gt;</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">label标签中的for属性值应当与相关元素中的id属性值相同</span><br><span class="line">② select下拉表单元素 双标签  </span><br><span class="line"></span><br></pre></td></tr></table></figure>  <select>  <option>选项1</option>  <option>选项2</option>  <option>选项3</option>  </select>  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt; select&gt;中至少包含一对&lt; option&gt;，可以在&lt; option&gt;标签中通过属性selected=&quot;selected&quot;表示当前项为默认选项</span><br><span class="line"></span><br><span class="line">③ textarea文本域元素 双标签  </span><br></pre></td></tr></table></figure>  <textarea rows="3" cols="20"> 页面一打开就会默认显示的内容</textarea>  ```    rows属性的属性值为"显示的行数",cols属性的属性值为"每行的字符数"（实际开发中使用css进行格式的设置）  </li></ul><h3 id="补充的标签"><a href="#补充的标签" class="headerlink" title="补充的标签"></a>补充的标签</h3><ul><li>&lt; button&gt; 搜索&lt; /button&gt;</li></ul>]]></content>
      
      
      <categories>
          
          <category> 前端学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前端 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow-3</title>
      <link href="/2022/01/26/tensorflow-3/"/>
      <url>/2022/01/26/tensorflow-3/</url>
      
        <content type="html"><![CDATA[<p> tensorflow 学习教程（三）</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><pre><code>本篇主要介绍Dataset相关的知识点,学习如何对数据进行读取、完成数据预处理后交给模型进行训练</code></pre><h1 id="Dataset-基础API使用"><a href="#Dataset-基础API使用" class="headerlink" title="Dataset 基础API使用"></a>Dataset 基础API使用</h1><ol><li>tf.data.Dataset.from_tensor_slices()<br>从内存中构造数据集，参数可以是普通的列表或者numpy的数组甚至可以是元组和字典<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = tf.data.Dataset.from_tensor_slices(np.arange(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>当输入参数是元组时:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">1.</span>,<span class="number">2.</span>],[<span class="number">3.</span>,<span class="number">4.</span>],[<span class="number">5.</span>,<span class="number">6.</span>]])</span><br><span class="line">y = np.array([<span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;cat&quot;</span>])</span><br><span class="line">dataset1 = tf.data.Dataset.from_tensor_slices((x,y)) <span class="comment"># 输入是一个元组(x,y)，这里x和y的长度必须是一致的</span></span><br></pre></td></tr></table></figure>当输入参数是字典时：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">1.</span>,<span class="number">2.</span>],[<span class="number">3.</span>,<span class="number">4.</span>],[<span class="number">5.</span>,<span class="number">6.</span>]])</span><br><span class="line">y = np.array([<span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;cat&quot;</span>])</span><br><span class="line">dataset1 = tf.data.Dataset.from_tensor_slices(&#123;<span class="string">&#x27;feature&#x27;</span>:x,<span class="string">&#x27;label&#x27;</span>:y&#125;) </span><br></pre></td></tr></table></figure></li><li> 对于构建好的数据集我们可以做些什么呢？</li></ol><ul><li>首先就是可以对数据集中的数据进行遍历<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(item) <span class="comment"># 得到的每一个元素都是一个tensor,共10个tensor</span></span><br></pre></td></tr></table></figure>对dataset1进行遍历时<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> item_x,item_y <span class="keyword">in</span> dataset1:</span><br><span class="line">    <span class="built_in">print</span>(item_x,item_y)</span><br></pre></td></tr></table></figure></li><li>在实际的机器学习问题中，针对数据集的几个常见的操作：<br>① repeat ：对数据集进行重复的读取（epoch的概念）<br>② batch : 每次从数据集中取多少的数据（batchsize的概念）<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = dataset.repeat(<span class="number">3</span>).batch(<span class="number">7</span>) <span class="comment"># 连环调用，产生一个新的dataset,同样可以对这个Dataset进行遍历，得到的4个tensor,前3个tensor长度为7，最后一个tensor长度不足7了，为2</span></span><br></pre></td></tr></table></figure>③ interleave : 对现有dataset中的每个元素进行处理并将结果进行汇总形成新的数据集<br>常见case:现有的dataset中存储的是一系列的文件名，需要将文件名取出并读取对应的文件内容<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset2 = dataset.interleave(</span><br><span class="line">    <span class="keyword">lambda</span> v: tf.data.Dataset.from_tensor_slices(v), <span class="comment"># map_fn</span></span><br><span class="line">    cycle_length = <span class="number">5</span>,</span><br><span class="line">    block_length = <span class="number">5</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>④ shuffle : 对现有的数据元素进行打乱，参数值越大则混乱程度越大<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = dataset.shuffle(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li></ul><p>在构建好数据集之后，可以在数据集上具体地调用.repeat() .batch() .interleave() .map() .shuffle() .list_files()等API</p><h1 id="使用Dataset读取csv文件"><a href="#使用Dataset读取csv文件" class="headerlink" title="使用Dataset读取csv文件"></a>使用Dataset读取csv文件</h1><p>csv文件是用逗号分隔的、按行存储的数据文件</p><h2 id="构造csv文件：将现有数据存成csv文件"><a href="#构造csv文件：将现有数据存成csv文件" class="headerlink" title="构造csv文件：将现有数据存成csv文件"></a>构造csv文件：将现有数据存成csv文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># np.c_[]可以将数据按行进行merge</span></span><br><span class="line">train_data = np.c_[x_train_scaled,y_train_scaled]</span><br><span class="line">test_data = np.c_[x_test_scaled,y_test_scaled]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output_dir = <span class="string">&quot;generate_csv&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">    os.mkdir(output_dir)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_csv</span>(<span class="params">output_dir,data,name_prefix,header = <span class="literal">None</span>,n_parts = <span class="number">10</span></span>):</span></span><br><span class="line">    path_format =os.path.join(output_dir,<span class="string">&quot;&#123;&#125;_&#123;:02d&#125;.csv&quot;</span>) </span><br><span class="line">    filenames = []</span><br><span class="line">    <span class="keyword">for</span> file_idx, row_indices <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.array_split(np.arange(<span class="built_in">len</span>(data)),n_parts)):</span><br><span class="line">        path_csv = path_format.<span class="built_in">format</span>(name_prefix,file_idx)</span><br><span class="line">        filenames.append(path_csv)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path_csv,<span class="string">&#x27;wt&#x27;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> header <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                f.write(header + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> row_index <span class="keyword">in</span> row_indices:</span><br><span class="line">                f.write(<span class="string">&#x27;,&#x27;</span>.join [<span class="built_in">repr</span>(col) <span class="keyword">for</span> col <span class="keyword">in</span> data[row_index]])</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> filenames</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">header_cols = housing.features_names + [<span class="string">&quot;Midian_prices&quot;</span>] <span class="comment"># housing是一个已经定义好的关于房价预测的数据集，这里header_cols就是一个列表，里边每个元素都是一个feature的名字，因为前边merge了label元素，所以加一列</span></span><br><span class="line">header_str = <span class="string">&quot;,&quot;</span>.join(header_cols)</span><br><span class="line">train_filenames = save_to_csv(output_dir,train_data,<span class="string">&quot;train&quot;</span>,header_str,n_parts = <span class="number">20</span>)</span><br><span class="line">test_filenames = save_to_csv(output_dir,test_data,<span class="string">&quot;tset&quot;</span>,headeer_str,n_parts =  <span class="number">10</span>)</span><br></pre></td></tr></table></figure><h2 id="使用API进行csv格式文件读取"><a href="#使用API进行csv格式文件读取" class="headerlink" title="使用API进行csv格式文件读取"></a>使用API进行csv格式文件读取</h2><p>steps如下：</p><ul><li>文件名 -&gt; dataset<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename_dataset = tf.data.Dataset.list_files(train_filenames) <span class="comment"># train_filenames是一个列表，里边每个元素都是训练集中的一个文件名，这个tf.data.Dataset.list_files()函数得到的是一个dataset，里边每个元素都是一个dtype = string的tensor，值为文件名</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>读取文件 -&gt; dataset -&gt; datasets -&gt; merge<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用interleave()方法，该方法对dataset中的每个元素进行遍历并进行某个操作，操作完成后的结果形成一个新的dataset</span></span><br><span class="line">n_readers = <span class="number">5</span></span><br><span class="line">dataset = filename_dataset.interleave(<span class="keyword">lambda</span> filename: tf.data.TextLineDataset(filename).skip(<span class="number">1</span>),<span class="comment"># tf.data.TextLineDataset这个API实现按行读取文本形成dataset,.skip()可以指定跳过前几行（这里我们是为了使header在数据集中不显示）</span></span><br><span class="line">cycle_length = n_readers</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li>从csv文件形成的数据集中的每个数据是几个数据值构成的字符串，因此需要利用tf.io.decode_csv(str,record_defaults)对csv文件进行解析,其中str是要解析的csv的字符串，record_defaults是每个字段的默认数据格式，record_defaults的长度必须和str中的字段个数保持一致<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sample_str = <span class="string">&#x27;1,2,3,4,5&#x27;</span> <span class="comment">#由,分隔的csv字符串</span></span><br><span class="line">record_defaults = [tf.constant(<span class="number">0</span>,dtype = tf.int32)] * <span class="number">5</span></span><br><span class="line">parsed_fields = tf.io.decode_csv(sample_str,record_defaults)</span><br><span class="line"><span class="comment"># 这样就将字符串转换成了tensor的列表</span></span><br></pre></td></tr></table></figure><h1 id="使用Dataset读取tfrecord文件"><a href="#使用Dataset读取tfrecord文件" class="headerlink" title="使用Dataset读取tfrecord文件"></a>使用Dataset读取tfrecord文件</h1>tfrecord是tensorflow中独有的存储数据的格式，tensorflow对这种格式的文件进行了优化，存储成这种格式的文件在处理的过程中会比较快<h2 id="构造tfrecord文件：将现有数据存成tfrecord格式"><a href="#构造tfrecord文件：将现有数据存成tfrecord格式" class="headerlink" title="构造tfrecord文件：将现有数据存成tfrecord格式"></a>构造tfrecord文件：将现有数据存成tfrecord格式</h2><h2 id="使用API进行tfrecord格式文件读取"><a href="#使用API进行tfrecord格式文件读取" class="headerlink" title="使用API进行tfrecord格式文件读取"></a>使用API进行tfrecord格式文件读取</h2>tf.train.FloatList<br>tf.train.Int64List<br>tf.train.BytesList</li></ul><p>tf.train.Feature<br>tf.train.Features<br>tf.train.Example<br>用来封装example写入record文件中</p><p>example.SerializeToString<br>序列化</p><p>tf.io.ParseSingleExample<br>解析一个具体的tf.example</p><p>tf.io.VarLenFeature<br>tf.io.FixedLenFeature<br>用来解析tf.example</p><p>tf.data.TFRecordDataset<br>基于tfrecord构建dataset<br>tf.io.TFRecordOptions<br>指定读取文件的类型</p><h2 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h2><ul><li>tf.constant()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.constant(value,dtype = <span class="literal">None</span>,shape = <span class="literal">None</span>,name = <span class="string">&#x27;const&#x27;</span>, verify_shape = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>该函数根据给出的value值创建常量Tensor,其中，value可以是数值（scalar）或者list，shape参数用来制定Tensor的形状，如果value是list，则len(value)一定要小于等于shape展开后的长度（当小于shape展开的长度时，不足的部分用list中最后一个元素补充）<br>以下t为定义好的Tensor<br>（1）可以使用index取元素<br>（2）操作：所有tensor的操作都可以完成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(t + <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.square(t))</span><br><span class="line"><span class="built_in">print</span>(t @tf.transpose(t))</span><br><span class="line">```  </span><br><span class="line">(<span class="number">3</span>)tensor和numpy转换</span><br><span class="line">```python</span><br><span class="line"><span class="built_in">print</span>(t.numpy()) <span class="comment"># 获得tensor的值</span></span><br><span class="line"><span class="built_in">print</span>(np.square(t)) <span class="comment"># 进行numpy操作 </span></span><br><span class="line">np_t = np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>]]) </span><br><span class="line"><span class="built_in">print</span>(tf.constant(np_t))  <span class="comment"># 将numpy转换为tensor</span></span><br></pre></td></tr></table></figure></li><li>tf.strings<br>向tf.constant()函数中传递的参数value为字符串时，生成的同样是tensor，只不过这个tensor的元素dtype = string，这样的tensor可以使用tf.strings下的方法对字符串进行操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = tf.constant(<span class="string">&quot;cafe&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t))</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t,unit = <span class="string">&quot;UTF8_CHAR&quot;</span>)) <span class="comment"># 获取utf-8编码下的字符串长度</span></span><br><span class="line"><span class="built_in">print</span>(tf.strings.unicode_decode(t,<span class="string">&quot;UTF8&quot;</span>)) <span class="comment"># 将字符串转换成utf-8</span></span><br></pre></td></tr></table></figure>同样可以使用tf.constant()函数存储字符串数组<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t =tf.constant([<span class="string">&quot;cafe&quot;</span>,<span class="string">&quot;coffee&quot;</span>,<span class="string">&quot;咖啡&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t,unit = <span class="string">&quot;UTF8_CHAR&quot;</span>)) <span class="comment"># 返回每个字符串的长度</span></span><br><span class="line"><span class="built_in">print</span>(tf.strings.unicode_decode(t,<span class="string">&quot;UTF8&quot;</span>))  <span class="comment"># 返回的是一个tf.RaggedTensor</span></span><br></pre></td></tr></table></figure></li><li>tf.ragged.constant()<br>tf.RaggedTensor数据类型是指在某个维度上长度不统一的tensor，是tensorflow2.0新加的功能，原因就是在实际应用中，很多数据可能本身就是不等长的（不规则的）<br>(1)索引操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = tf.ragged.constant([[<span class="number">12</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>],[<span class="number">6</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(r[<span class="number">1</span>]) <span class="comment"># 取第一行返回的就是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(r[<span class="number">1</span>:<span class="number">2</span>]) <span class="comment"># 取前两行返回的就是一个RaggedTensor</span></span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>(2)其他支持的操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#拼接操作</span></span><br><span class="line">r2 = tf.ragged.constant([[<span class="number">24</span>,<span class="number">65</span>],[],[<span class="number">54</span>,<span class="number">55</span>,<span class="number">86</span>]])</span><br><span class="line"><span class="built_in">print</span>(tf.concat([r,r2],axis = <span class="number">0</span>)) <span class="comment"># 在特定的维度上进行拼接（两个拼接的tensor要在形状上满足一定的条件能够进行拼接才行）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># to_tensor()操作</span></span><br><span class="line"><span class="built_in">print</span>(r.to_tensor()) <span class="comment"># 返回类型是普通的tensor，按照最大的长度，对于长度不足的维度在最后补0</span></span><br></pre></td></tr></table></figure></li><li>tf.SparseTensor()<br>如果我们不希望得到像to_tensor()这个方法一样所有的0都是补在最后边，那么就需要用到这里的稀疏矩阵的定义了，将非零元素的位置和数值记录下来就好了  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = tf.SparseTensor(indices = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">3</span>]],values = [<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],dense_shape = =[<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># indices必须是排好序的，不然调用tf.sparse.to_dense()方法时会出错，如果indices不是排好序的，那么需要使用s1 = tf.sparse.reorder(s)进行排序，然后对s1调用tf.sparse/to_dense()方法即可</span></span><br></pre></td></tr></table></figure>（1）支持的操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.sparse.to_dense(s))  <span class="comment"># 类似于RaggedTensor的to_tesor()方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 和标量进行乘法</span></span><br><span class="line">s2 = s*<span class="number">2.0</span></span><br><span class="line"><span class="comment"># 将sparse矩阵和密集矩阵进行乘法</span></span><br><span class="line">s3 = tf.constant([[<span class="number">10.</span>,<span class="number">20.</span>],[<span class="number">30.</span>,<span class="number">40.</span>],[<span class="number">50.</span>,<span class="number">60.</span>],[<span class="number">70.</span>,<span class="number">80.</span>]])</span><br><span class="line"><span class="built_in">print</span>(tf.sparse.sparse_dense_matmul(s,s3))</span><br><span class="line"><span class="comment"># 不支持加法，如下会报错</span></span><br><span class="line"><span class="built_in">print</span>(s+<span class="number">1.</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2>以上常量的值是不能变的，但是在模型的参数训练过程中，参数的值是需要改变的，因此就需要变量  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.Variable([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>],[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(v.value()) <span class="comment"># 将Variable变成一个Tensor</span></span><br><span class="line"><span class="built_in">print</span>(v.numpy()) <span class="comment">#像常量一样得到取值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变量和常量的操作大致一致，但是变量可以重新赋值(只能用assign()函数而不能用等于号)</span></span><br><span class="line">v.assign(<span class="number">2</span>*v) <span class="comment">#对变量整体赋值</span></span><br><span class="line">v[<span class="number">0</span>,<span class="number">1</span>].assign(<span class="number">42</span>) <span class="comment"># 对变量中的某个位置的值进行赋值</span></span><br><span class="line">v[<span class="number">1</span>].assign([<span class="number">4.</span>,<span class="number">3.</span>,<span class="number">2.</span>]) <span class="comment"># 对变量的某一行重新赋值</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h1>steps:</li><li>首先定义一个函数<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">customized_mse</span>(<span class="params">y_true,y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(y_pred - y_true)) </span><br></pre></td></tr></table></figure></li><li>然后在model.compile()中设置loss参数等于损失函数名<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss = customized_mse,optimizer = <span class="string">&quot;sgd&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="自定义层次"><a href="#自定义层次" class="headerlink" title="自定义层次"></a>自定义层次</h1>(1)layer的重要属性</li><li>layer.variables 返回layer的所有参数（变量）</li><li>layer.trainable_variables 返回所有的可训练参数（变量）</li><li>可以使用help(layer)获取layer的方法和属性<br>（2）使用前述继承类（子类）的方法实现自定义<br>需要重载__init__()、build()、call()函数<br>(3)当使用继承类的方法实现自定义层时，如果层不需要参数，则使用上述方法会显得很重，因此可以使用类似于python中常用的lambda函数(keras.layers.Lambda())的方法进行层的定义<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">customized_softplus = keras.layers.Lambda(<span class="keyword">lambda</span> x:tf.nn.softplus(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用的时候直接给customized_softplus()函数传参数作为x</span></span><br><span class="line"><span class="built_in">print</span>(customized_softplus([[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>],[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]]))</span><br><span class="line">或者在model中直接model = keras.models.Sequential([...,...,customized_softplus,...])</span><br></pre></td></tr></table></figure></li></ul><h1 id="tf-function的使用"><a href="#tf-function的使用" class="headerlink" title="@tf.function的使用"></a>@tf.function的使用</h1><p>是tensorflow2.0的很重要的feature，能够：</p><ul><li>将python函数（if 语句、while语句、for 语句…）编译成图,可以使用python的语法进行模型的构建,并使用tensorflow中实现的优化，速度上会有优势</li><li>易于将模型导出为GraphDef+checkpoint或者SavedModel</li><li>使得eager execution可以默认打开（可以保存模型中间的训练结果了，不然的话就相当于只能训练而没有办法通过保存先前的图结构进而进行预测了）</li><li>tf1.0的代码可以通过@tf.function的封装在tf2.0中继续使用<br>(1)使用tf.function()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaled_elu</span>(<span class="params">z,scale = <span class="number">1.0</span>,alpha = <span class="number">1.0</span></span>):</span> <span class="comment"># 这里输入的z如果是向量，则对z中的向量元素分别做操作然后返回向量，这也是普通的python函数无法实现的</span></span><br><span class="line">    is_positive = tf.greater_equal(z,<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">return</span> scale * tf.where(is_positive,z,alpha*tf.nn.elu(z))<span class="comment"># tensorflow中使用tf.where实现三元表达式？：</span></span><br><span class="line">scaled_elu_tf = tf.function(scaled_elu) <span class="comment"># 编译成图</span></span><br><span class="line"><span class="built_in">print</span>(scaled_elu_tf.python_function <span class="keyword">is</span> scaled_elu) <span class="comment">#通过.python_function转回python函数，返回结果是True</span></span><br></pre></td></tr></table></figure>(2)使用@tf.function<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">converge_to_2</span>(<span class="params">n_iters</span>):</span></span><br><span class="line">    total = tf.constant(<span class="number">0.</span>)</span><br><span class="line">    increment = tf.constant(<span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> n_iters:</span><br><span class="line">        total += increment</span><br><span class="line">        increment /=<span class="number">2.0</span></span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure><h1 id="autograph是实现tf-function从python函数到tensorflow中的图的机制"><a href="#autograph是实现tf-function从python函数到tensorflow中的图的机制" class="headerlink" title="autograph是实现tf.function从python函数到tensorflow中的图的机制"></a>autograph是实现tf.function从python函数到tensorflow中的图的机制</h1>(1)使用tf.autograph.to_code()将python函数转换成tensorflow代码并转换成图结构</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tf_code</span>(<span class="params">func</span>):</span></span><br><span class="line">    code = tf.autograph.to_code(func)</span><br><span class="line">    <span class="keyword">from</span> IPython.dispaly <span class="keyword">import</span> display,Markdown</span><br><span class="line">    display(Markdown(<span class="string">&#x27;```python\n&#123;&#125;\n```&#x27;</span>.<span class="built_in">format</span>(code))) <span class="comment">#使用markdown语法将code显示</span></span><br><span class="line"></span><br><span class="line">display_tf_code(tf_scaled_elu)</span><br><span class="line">display_tf_code(converge_to_2)</span><br></pre></td></tr></table></figure><p>(2)tf.autograph.to_graph()将代码转换成图（类似上边）<br>(3)在定义神经网络时更多的是使用Variable,在函数中使用Variable（需要在函数外边声明tf.Variable()）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">0.</span>) <span class="comment"># 这个variable如果放在函数内部去定义的话会报错</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_21</span>():</span></span><br><span class="line">    retrun var.assign_add(<span class="number">21</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(add_21())</span><br></pre></td></tr></table></figure><p>(4)python 是一门弱类型语言，当我们希望对输入的数据类型进行限制时需要在@tf.function()中添加参数input_signature实现函数签名</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function(<span class="params">input_signature =  tf. TensorSpec(<span class="params">[<span class="literal">None</span>],tf.int32,name = <span class="string">&quot;x&quot;</span></span>)</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">converge_to_2</span>(<span class="params">n_iters</span>):</span></span><br><span class="line">    total = tf.constant(<span class="number">0.</span>)</span><br><span class="line">    increment = tf.constant(<span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> n_iters:</span><br><span class="line">        total += increment</span><br><span class="line">        increment /=<span class="number">2.0</span></span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure><p>在有了input_signature参数之后，在tensorflo中才能被保存为saved model,在被保存为saved model 的过程中，需要使用get_concrete_function的函数将有@tf.function()标注的python函数变为带有图定义的函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @tf.function:py func -&gt; tf graph</span></span><br><span class="line"><span class="comment"># get_concrete_function +add input_signature -&gt; SavedModel</span></span><br><span class="line">converge_to_2_int32 = converge_to_2.concrete_get_function(tf.TensorSpec([<span class="literal">None</span>],tf.int32,name = <span class="string">&quot;x&quot;</span>))</span><br><span class="line"><span class="comment"># 注意这个TensorSpec参数也可以传入具体的数值：converge_to_2_int32 = converge_to_2.concrete_get_function(tf.TensorSpec([5],tf.int32))或者converge_to_2_int32 = converge_to_2.concrete_get_function(tf.costant([1,2,3]))</span></span><br></pre></td></tr></table></figure><p>对于以上的函数converge_to_2_int32可以调用.graph函数得到图结构对象，同时使用get_operations等方法获取函数中的操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">converge_to_2_int32.gragh</span><br><span class="line">conveerge_to_2_int32.graph.get_operations() <span class="comment"># 得到操作的列表</span></span><br><span class="line">converge_to_2_int32.graph.get_operaions()[<span class="number">2</span>] <span class="comment"># 得到列表中的某个元素即某个操作</span></span><br><span class="line">converge_to_2_int32.graph.get_operation_by_name(<span class="string">&quot;x&quot;</span>) <span class="comment"># 通过名字获取操作</span></span><br><span class="line">converge_to_2_int32.graph.get_tensor_by_name(<span class="string">&quot;x:0&quot;</span>) <span class="comment"># 通过名字获取tensor</span></span><br><span class="line">converge_to_2_int32.as_graph_def() <span class="comment"># 将graph_def打印出来得到图的详细信息</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="自定义求导"><a href="#自定义求导" class="headerlink" title="自定义求导"></a>自定义求导</h1><p>tf.GradientTape()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x1 = tape.gradient(z,x1) <span class="comment"># 在tensorflow中tape只能用一次，被调用一次gradient之后tape对象就会被释放，因此再计算dz_x2就会报错</span></span><br><span class="line">    dz_x2 =  tape.gradient(z,x2) <span class="comment">#报错</span></span><br></pre></td></tr></table></figure><p>如何解决呢？ =》 在定义tape的时候设置persistent参数为True，使得该对象是可保存的,但是要注意这个对象使用完毕之后要自己主动删掉</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x1 = tape.gradient(z,x1) </span><br><span class="line">    dz_x2 =  tape.gradient(z,x2) <span class="comment">#可以多次调用tape</span></span><br><span class="line"><span class="keyword">del</span> tape <span class="comment"># 删掉tape</span></span><br></pre></td></tr></table></figure><p>如何同时将x1和x2的偏导都求出来呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x =  tape.gradient(z,[x1,x2]) <span class="comment"># 将x1和x2同时放在一个列表中传参给tape.gradient</span></span><br></pre></td></tr></table></figure><p>能不能对前边提到的tf.constant()定义的常量求偏导呢？ =》像对变量一样进行操作得到的结果为[None,None] =&gt; 如果确实需要对某些常量进行导数的关注怎么办呢？ =》通过对tape进行一些操作可以设置需要关注的常量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x1和x2是用tf.constant()定义的两个常量</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch(x1)</span><br><span class="line">    tape.watch(x2)</span><br><span class="line">    z = g(x1,x2)</span><br><span class="line">    dx_x =tape.gradient(z,[x1,x2])</span><br></pre></td></tr></table></figure><p>有没有可能有两个目标函数，同时对同一个变量进行求导呢？=》得到的是两个目标函数对变量导数值的和</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z1 = <span class="number">3</span>*x</span><br><span class="line">    z2 =x**<span class="number">2</span></span><br><span class="line">    tape.gradient([z1,z2],x)<span class="comment"># 得到的是z1和z2对x导数的和</span></span><br></pre></td></tr></table></figure><p>接下来使用嵌套的tape实现二阶求导~</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">4.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> outer_tape:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> inner_tape:</span><br><span class="line">        z = g(x1,x2)</span><br><span class="line">    inner_gradients = inner_tape.gradient(z,[x1,x2]) </span><br><span class="line">outer_gradients = outer_tape.gradient(inner_gradient,[x1,x2]) <span class="keyword">for</span> inner_gradient <span class="keyword">in</span> inner_gradients</span><br><span class="line"><span class="built_in">print</span>(outer_gradients) <span class="comment"># 得到的是2*2的对角矩阵</span></span><br><span class="line"><span class="keyword">del</span> inner_tape</span><br><span class="line"><span class="keyword">del</span> outer_tape</span><br></pre></td></tr></table></figure><p>在得到导数之后，就可以使用梯度进一步进行梯度下降更新参数了，如何将GradientTape与keras里边的optimizer结合使用呢？ =&gt; optimizer.apply_gradients()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">optimizer = keras.optimizers.SGD(lr = learning_rate)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        z = f(x)</span><br><span class="line">    dz_dx = tape.gradient(z,x)</span><br><span class="line">    optimizer.apply_gradients([(dz_dx,x)]) <span class="comment"># 参数是一个列表，列表里边是一个个的pair，pair中分别对应梯度（在前）和参数（在后）</span></span><br></pre></td></tr></table></figure><p>只要能够定义出目标函数，就能使用tf.GradientTape求梯度，并结合tf.optimizers去进行梯度更新参数优化</p>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow-2</title>
      <link href="/2022/01/24/tensorflow-2/"/>
      <url>/2022/01/24/tensorflow-2/</url>
      
        <content type="html"><![CDATA[<p> tensorflow 学习教程（二）</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><pre><code>在tensorflow 学习教程（一）对于Layer和Model进行一定的介绍之后，我们大致了解了使用tensorflow框架构建模型的大致流程和基本逻辑，在本篇教程中，我们进一步对tf的基础API进行学习。</code></pre><h1 id="基础数据类型"><a href="#基础数据类型" class="headerlink" title="基础数据类型"></a>基础数据类型</h1><h2 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h2><ul><li>tf.constant()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.constant(value,dtype = <span class="literal">None</span>,shape = <span class="literal">None</span>,name = <span class="string">&#x27;const&#x27;</span>, verify_shape = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>该函数根据给出的value值创建常量Tensor,其中，value可以是数值（scalar）或者list，shape参数用来制定Tensor的形状，如果value是list，则len(value)一定要小于等于shape展开后的长度（当小于shape展开的长度时，不足的部分用list中最后一个元素补充）<br>以下t为定义好的Tensor<br>（1）可以使用index取元素<br>（2）操作：所有tensor的操作都可以完成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(t + <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.square(t))</span><br><span class="line"><span class="built_in">print</span>(t @tf.transpose(t))</span><br><span class="line">```  </span><br><span class="line">(<span class="number">3</span>)tensor和numpy转换</span><br><span class="line">```python</span><br><span class="line"><span class="built_in">print</span>(t.numpy()) <span class="comment"># 获得tensor的值</span></span><br><span class="line"><span class="built_in">print</span>(np.square(t)) <span class="comment"># 进行numpy操作 </span></span><br><span class="line">np_t = np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>]]) </span><br><span class="line"><span class="built_in">print</span>(tf.constant(np_t))  <span class="comment"># 将numpy转换为tensor</span></span><br></pre></td></tr></table></figure></li><li>tf.strings<br>向tf.constant()函数中传递的参数value为字符串时，生成的同样是tensor，只不过这个tensor的元素dtype = string，这样的tensor可以使用tf.strings下的方法对字符串进行操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = tf.constant(<span class="string">&quot;cafe&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t))</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t,unit = <span class="string">&quot;UTF8_CHAR&quot;</span>)) <span class="comment"># 获取utf-8编码下的字符串长度</span></span><br><span class="line"><span class="built_in">print</span>(tf.strings.unicode_decode(t,<span class="string">&quot;UTF8&quot;</span>)) <span class="comment"># 将字符串转换成utf-8</span></span><br></pre></td></tr></table></figure>同样可以使用tf.constant()函数存储字符串数组<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t =tf.constant([<span class="string">&quot;cafe&quot;</span>,<span class="string">&quot;coffee&quot;</span>,<span class="string">&quot;咖啡&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(tf.strings.length(t,unit = <span class="string">&quot;UTF8_CHAR&quot;</span>)) <span class="comment"># 返回每个字符串的长度</span></span><br><span class="line"><span class="built_in">print</span>(tf.strings.unicode_decode(t,<span class="string">&quot;UTF8&quot;</span>))  <span class="comment"># 返回的是一个tf.RaggedTensor</span></span><br></pre></td></tr></table></figure></li><li>tf.ragged.constant()<br>tf.RaggedTensor数据类型是指在某个维度上长度不统一的tensor，是tensorflow2.0新加的功能，原因就是在实际应用中，很多数据可能本身就是不等长的（不规则的）<br>(1)索引操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = tf.ragged.constant([[<span class="number">12</span>,<span class="number">2</span>],[<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>],[<span class="number">6</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(r[<span class="number">1</span>]) <span class="comment"># 取第一行返回的就是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(r[<span class="number">1</span>:<span class="number">2</span>]) <span class="comment"># 取前两行返回的就是一个RaggedTensor</span></span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>(2)其他支持的操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#拼接操作</span></span><br><span class="line">r2 = tf.ragged.constant([[<span class="number">24</span>,<span class="number">65</span>],[],[<span class="number">54</span>,<span class="number">55</span>,<span class="number">86</span>]])</span><br><span class="line"><span class="built_in">print</span>(tf.concat([r,r2],axis = <span class="number">0</span>)) <span class="comment"># 在特定的维度上进行拼接（两个拼接的tensor要在形状上满足一定的条件能够进行拼接才行）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># to_tensor()操作</span></span><br><span class="line"><span class="built_in">print</span>(r.to_tensor()) <span class="comment"># 返回类型是普通的tensor，按照最大的长度，对于长度不足的维度在最后补0</span></span><br></pre></td></tr></table></figure></li><li>tf.SparseTensor()<br>如果我们不希望得到像to_tensor()这个方法一样所有的0都是补在最后边，那么就需要用到这里的稀疏矩阵的定义了，将非零元素的位置和数值记录下来就好了  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = tf.SparseTensor(indices = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">3</span>]],values = [<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],dense_shape = =[<span class="number">3</span>,<span class="number">4</span>]) <span class="comment"># indices必须是排好序的，不然调用tf.sparse.to_dense()方法时会出错，如果indices不是排好序的，那么需要使用s1 = tf.sparse.reorder(s)进行排序，然后对s1调用tf.sparse/to_dense()方法即可</span></span><br></pre></td></tr></table></figure>（1）支持的操作<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.sparse.to_dense(s))  <span class="comment"># 类似于RaggedTensor的to_tesor()方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 和标量进行乘法</span></span><br><span class="line">s2 = s*<span class="number">2.0</span></span><br><span class="line"><span class="comment"># 将sparse矩阵和密集矩阵进行乘法</span></span><br><span class="line">s3 = tf.constant([[<span class="number">10.</span>,<span class="number">20.</span>],[<span class="number">30.</span>,<span class="number">40.</span>],[<span class="number">50.</span>,<span class="number">60.</span>],[<span class="number">70.</span>,<span class="number">80.</span>]])</span><br><span class="line"><span class="built_in">print</span>(tf.sparse.sparse_dense_matmul(s,s3))</span><br><span class="line"><span class="comment"># 不支持加法，如下会报错</span></span><br><span class="line"><span class="built_in">print</span>(s+<span class="number">1.</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2>以上常量的值是不能变的，但是在模型的参数训练过程中，参数的值是需要改变的，因此就需要变量  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.Variable([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>],[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(v.value()) <span class="comment"># 将Variable变成一个Tensor</span></span><br><span class="line"><span class="built_in">print</span>(v.numpy()) <span class="comment">#像常量一样得到取值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变量和常量的操作大致一致，但是变量可以重新赋值(只能用assign()函数而不能用等于号)</span></span><br><span class="line">v.assign(<span class="number">2</span>*v) <span class="comment">#对变量整体赋值</span></span><br><span class="line">v[<span class="number">0</span>,<span class="number">1</span>].assign(<span class="number">42</span>) <span class="comment"># 对变量中的某个位置的值进行赋值</span></span><br><span class="line">v[<span class="number">1</span>].assign([<span class="number">4.</span>,<span class="number">3.</span>,<span class="number">2.</span>]) <span class="comment"># 对变量的某一行重新赋值</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h1>steps:</li><li>首先定义一个函数<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">customized_mse</span>(<span class="params">y_true,y_pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(y_pred - y_true)) </span><br></pre></td></tr></table></figure></li><li>然后在model.compile()中设置loss参数等于损失函数名<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss = customized_mse,optimizer = <span class="string">&quot;sgd&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="自定义层次"><a href="#自定义层次" class="headerlink" title="自定义层次"></a>自定义层次</h1>(1)layer的重要属性</li><li>layer.variables 返回layer的所有参数（变量）</li><li>layer.trainable_variables 返回所有的可训练参数（变量）</li><li>可以使用help(layer)获取layer的方法和属性<br>（2）使用前述继承类（子类）的方法实现自定义<br>需要重载__init__()、build()、call()函数<br>(3)当使用继承类的方法实现自定义层时，如果层不需要参数，则使用上述方法会显得很重，因此可以使用类似于python中常用的lambda函数(keras.layers.Lambda())的方法进行层的定义<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">customized_softplus = keras.layers.Lambda(<span class="keyword">lambda</span> x:tf.nn.softplus(x))</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用的时候直接给customized_softplus()函数传参数作为x</span></span><br><span class="line"><span class="built_in">print</span>(customized_softplus([[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>],[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]]))</span><br><span class="line">或者在model中直接model = keras.models.Sequential([...,...,customized_softplus,...])</span><br></pre></td></tr></table></figure></li></ul><h1 id="tf-function的使用"><a href="#tf-function的使用" class="headerlink" title="@tf.function的使用"></a>@tf.function的使用</h1><p>是tensorflow2.0的很重要的feature，能够：</p><ul><li>将python函数（if 语句、while语句、for 语句…）编译成图,可以使用python的语法进行模型的构建,并使用tensorflow中实现的优化，速度上会有优势</li><li>易于将模型导出为GraphDef+checkpoint或者SavedModel</li><li>使得eager execution可以默认打开（可以保存模型中间的训练结果了，不然的话就相当于只能训练而没有办法通过保存先前的图结构进而进行预测了）</li><li>tf1.0的代码可以通过@tf.function的封装在tf2.0中继续使用<br>(1)使用tf.function()<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaled_elu</span>(<span class="params">z,scale = <span class="number">1.0</span>,alpha = <span class="number">1.0</span></span>):</span> <span class="comment"># 这里输入的z如果是向量，则对z中的向量元素分别做操作然后返回向量，这也是普通的python函数无法实现的</span></span><br><span class="line">    is_positive = tf.greater_equal(z,<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">return</span> scale * tf.where(is_positive,z,alpha*tf.nn.elu(z))<span class="comment"># tensorflow中使用tf.where实现三元表达式？：</span></span><br><span class="line">scaled_elu_tf = tf.function(scaled_elu) <span class="comment"># 编译成图</span></span><br><span class="line"><span class="built_in">print</span>(scaled_elu_tf.python_function <span class="keyword">is</span> scaled_elu) <span class="comment">#通过.python_function转回python函数，返回结果是True</span></span><br></pre></td></tr></table></figure>(2)使用@tf.function<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">converge_to_2</span>(<span class="params">n_iters</span>):</span></span><br><span class="line">    total = tf.constant(<span class="number">0.</span>)</span><br><span class="line">    increment = tf.constant(<span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> n_iters:</span><br><span class="line">        total += increment</span><br><span class="line">        increment /=<span class="number">2.0</span></span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure><h1 id="autograph是实现tf-function从python函数到tensorflow中的图的机制"><a href="#autograph是实现tf-function从python函数到tensorflow中的图的机制" class="headerlink" title="autograph是实现tf.function从python函数到tensorflow中的图的机制"></a>autograph是实现tf.function从python函数到tensorflow中的图的机制</h1>(1)使用tf.autograph.to_code()将python函数转换成tensorflow代码并转换成图结构</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_tf_code</span>(<span class="params">func</span>):</span></span><br><span class="line">    code = tf.autograph.to_code(func)</span><br><span class="line">    <span class="keyword">from</span> IPython.dispaly <span class="keyword">import</span> display,Markdown</span><br><span class="line">    display(Markdown(<span class="string">&#x27;```python\n&#123;&#125;\n```&#x27;</span>.<span class="built_in">format</span>(code))) <span class="comment">#使用markdown语法将code显示</span></span><br><span class="line"></span><br><span class="line">display_tf_code(tf_scaled_elu)</span><br><span class="line">display_tf_code(converge_to_2)</span><br></pre></td></tr></table></figure><p>(2)tf.autograph.to_graph()将代码转换成图（类似上边）<br>(3)在定义神经网络时更多的是使用Variable,在函数中使用Variable（需要在函数外边声明tf.Variable()）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">var = tf.Variable(<span class="number">0.</span>) <span class="comment"># 这个variable如果放在函数内部去定义的话会报错</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_21</span>():</span></span><br><span class="line">    retrun var.assign_add(<span class="number">21</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(add_21())</span><br></pre></td></tr></table></figure><p>(4)python 是一门弱类型语言，当我们希望对输入的数据类型进行限制时需要在@tf.function()中添加参数input_signature实现函数签名</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@tf.function(<span class="params">input_signature =  tf. TensorSpec(<span class="params">[<span class="literal">None</span>],tf.int32,name = <span class="string">&quot;x&quot;</span></span>)</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">converge_to_2</span>(<span class="params">n_iters</span>):</span></span><br><span class="line">    total = tf.constant(<span class="number">0.</span>)</span><br><span class="line">    increment = tf.constant(<span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> n_iters:</span><br><span class="line">        total += increment</span><br><span class="line">        increment /=<span class="number">2.0</span></span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure><p>在有了input_signature参数之后，在tensorflo中才能被保存为saved model,在被保存为saved model 的过程中，需要使用get_concrete_function的函数将有@tf.function()标注的python函数变为带有图定义的函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># @tf.function:py func -&gt; tf graph</span></span><br><span class="line"><span class="comment"># get_concrete_function +add input_signature -&gt; SavedModel</span></span><br><span class="line">converge_to_2_int32 = converge_to_2.concrete_get_function(tf.TensorSpec([<span class="literal">None</span>],tf.int32,name = <span class="string">&quot;x&quot;</span>))</span><br><span class="line"><span class="comment"># 注意这个TensorSpec参数也可以传入具体的数值：converge_to_2_int32 = converge_to_2.concrete_get_function(tf.TensorSpec([5],tf.int32))或者converge_to_2_int32 = converge_to_2.concrete_get_function(tf.costant([1,2,3]))</span></span><br></pre></td></tr></table></figure><p>对于以上的函数converge_to_2_int32可以调用.graph函数得到图结构对象，同时使用get_operations等方法获取函数中的操作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">converge_to_2_int32.gragh</span><br><span class="line">conveerge_to_2_int32.graph.get_operations() <span class="comment"># 得到操作的列表</span></span><br><span class="line">converge_to_2_int32.graph.get_operaions()[<span class="number">2</span>] <span class="comment"># 得到列表中的某个元素即某个操作</span></span><br><span class="line">converge_to_2_int32.graph.get_operation_by_name(<span class="string">&quot;x&quot;</span>) <span class="comment"># 通过名字获取操作</span></span><br><span class="line">converge_to_2_int32.graph.get_tensor_by_name(<span class="string">&quot;x:0&quot;</span>) <span class="comment"># 通过名字获取tensor</span></span><br><span class="line">converge_to_2_int32.as_graph_def() <span class="comment"># 将graph_def打印出来得到图的详细信息</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="自定义求导"><a href="#自定义求导" class="headerlink" title="自定义求导"></a>自定义求导</h1><p>tf.GradientTape()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x1 = tape.gradient(z,x1) <span class="comment"># 在tensorflow中tape只能用一次，被调用一次gradient之后tape对象就会被释放，因此再计算dz_x2就会报错</span></span><br><span class="line">    dz_x2 =  tape.gradient(z,x2) <span class="comment">#报错</span></span><br></pre></td></tr></table></figure><p>如何解决呢？ =》 在定义tape的时候设置persistent参数为True，使得该对象是可保存的,但是要注意这个对象使用完毕之后要自己主动删掉</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x1 = tape.gradient(z,x1) </span><br><span class="line">    dz_x2 =  tape.gradient(z,x2) <span class="comment">#可以多次调用tape</span></span><br><span class="line"><span class="keyword">del</span> tape <span class="comment"># 删掉tape</span></span><br></pre></td></tr></table></figure><p>如何同时将x1和x2的偏导都求出来呢？</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradinetTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z = g(x1,x2) <span class="comment"># g是提前定义好的关于x1和x2的函数</span></span><br><span class="line">    dz_x =  tape.gradient(z,[x1,x2]) <span class="comment"># 将x1和x2同时放在一个列表中传参给tape.gradient</span></span><br></pre></td></tr></table></figure><p>能不能对前边提到的tf.constant()定义的常量求偏导呢？ =》像对变量一样进行操作得到的结果为[None,None] =&gt; 如果确实需要对某些常量进行导数的关注怎么办呢？ =》通过对tape进行一些操作可以设置需要关注的常量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x1和x2是用tf.constant()定义的两个常量</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch(x1)</span><br><span class="line">    tape.watch(x2)</span><br><span class="line">    z = g(x1,x2)</span><br><span class="line">    dx_x =tape.gradient(z,[x1,x2])</span><br></pre></td></tr></table></figure><p>有没有可能有两个目标函数，同时对同一个变量进行求导呢？=》得到的是两个目标函数对变量导数值的和</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">5.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    z1 = <span class="number">3</span>*x</span><br><span class="line">    z2 =x**<span class="number">2</span></span><br><span class="line">    tape.gradient([z1,z2],x)<span class="comment"># 得到的是z1和z2对x导数的和</span></span><br></pre></td></tr></table></figure><p>接下来使用嵌套的tape实现二阶求导~</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x1 = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">x2 = tf.Variable(<span class="number">4.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> outer_tape:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape(persistent = <span class="literal">True</span>) <span class="keyword">as</span> inner_tape:</span><br><span class="line">        z = g(x1,x2)</span><br><span class="line">    inner_gradients = inner_tape.gradient(z,[x1,x2]) </span><br><span class="line">outer_gradients = outer_tape.gradient(inner_gradient,[x1,x2]) <span class="keyword">for</span> inner_gradient <span class="keyword">in</span> inner_gradients</span><br><span class="line"><span class="built_in">print</span>(outer_gradients) <span class="comment"># 得到的是2*2的对角矩阵</span></span><br><span class="line"><span class="keyword">del</span> inner_tape</span><br><span class="line"><span class="keyword">del</span> outer_tape</span><br></pre></td></tr></table></figure><p>在得到导数之后，就可以使用梯度进一步进行梯度下降更新参数了，如何将GradientTape与keras里边的optimizer结合使用呢？ =&gt; optimizer.apply_gradients()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">optimizer = keras.optimizers.SGD(lr = learning_rate)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        z = f(x)</span><br><span class="line">    dz_dx = tape.gradient(z,x)</span><br><span class="line">    optimizer.apply_gradients([(dz_dx,x)]) <span class="comment"># 参数是一个列表，列表里边是一个个的pair，pair中分别对应梯度（在前）和参数（在后）</span></span><br></pre></td></tr></table></figure><p>只要能够定义出目标函数，就能使用tf.GradientTape求梯度，并结合tf.optimizers去进行梯度更新参数优化</p>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow-1</title>
      <link href="/2022/01/20/tensorflow-1/"/>
      <url>/2022/01/20/tensorflow-1/</url>
      
        <content type="html"><![CDATA[<p> tensorflow 学习教程（一）</p><h1 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h1><pre><code>本篇学习教程主要针对tensorflow2.0,同时兼顾tensorflow1.0的部分代码特点进行总结。今天主要学习了tensorflow中关于layer的定义，包括函数式/功能式API的概念及使用，子类API的概念及使用,以及Model与Layer的区别。下面就让我们逐一对这些内容进行了解吧！</code></pre><h1 id="函数式-功能式API"><a href="#函数式-功能式API" class="headerlink" title="函数式/功能式API"></a>函数式/功能式API</h1><h2 id="初识函数式API"><a href="#初识函数式API" class="headerlink" title="初识函数式API"></a>初识函数式API</h2><p>函数式API（function API） 有时又称作功能式API，在这种方式下，我们写好的model的layer就像一个被写好的普通函数一样可以通过向函数传参（该层所要处理的数据）的方式被直接调用，增加了程序编写的灵活性。  </p><p>一个全连接网络可能包含以下几个部分：输入层、隐层、输出层，有时还需要层间进行一个concatenate，我们现在就以这几种layer为例，看一下怎么以函数式API的方式进行编写。</p><ul><li>输入层  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_result = keras.layers.Input(shape = x_train.shape[<span class="number">1</span>:]) </span><br></pre></td></tr></table></figure></li><li>隐藏层<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden = keras.layers.Dense(<span class="number">30</span>,activation = <span class="string">&#x27;relu&#x27;</span>) <span class="comment"># 定义函数</span></span><br><span class="line">hidden_result = hidden(input_result) <span class="comment"># 函数调用</span></span><br></pre></td></tr></table></figure></li><li>concatenate层<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">concat = keras.layers.concatenate([input_result,hidden_result]) </span><br></pre></td></tr></table></figure></li><li>输出层<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">output_result  = output(concat)</span><br></pre></td></tr></table></figure></li><li>将上述几层连接起来形成一个model<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.models.Model(inputs=[input_result],outputs=[output_result])</span><br></pre></td></tr></table></figure></li><li>查看model<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.sumamry() </span><br></pre></td></tr></table></figure><img src="./tensorflow/tensorflow-1/1.png" alt="model.summary()"></li><li><em>同时，所有的模型也能像层一样被调用</em>*<h2 id="函数式API训练、估计和推断"><a href="#函数式API训练、估计和推断" class="headerlink" title="函数式API训练、估计和推断"></a>函数式API训练、估计和推断</h2>同Sequential :<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss = keras.losses.SparseCategoricalCrossEntropy(from_logits = <span class="literal">True</span>),  </span><br><span class="line">    optimizer = keras.optimizers.RMSprop(),  </span><br><span class="line">    metrics=[<span class="string">&quot;accuracy&quot;</span>],)</span><br><span class="line">model.fit(x_train,y_train,epochs = <span class="number">20</span>,batch_size = <span class="number">64</span>,validation_split = <span class="number">0.2</span>)</span><br><span class="line">model.evaluate(x_test,y_test,verbose = <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h2>同Sequential:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;文件路径&#x27;</span>)</span><br><span class="line">keras.models.load_model(<span class="string">&quot;文件路径&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="函数式API应用举例"><a href="#函数式API应用举例" class="headerlink" title="函数式API应用举例"></a>函数式API应用举例</h2><h3 id="实现模型的多输入与多输出"><a href="#实现模型的多输入与多输出" class="headerlink" title="实现模型的多输入与多输出"></a>实现模型的多输入与多输出</h3></li></ul><ol><li>函数式API使得多输入与多输出的应用变得简单，而Sequential API难以做到。<br>主要就是通过上述构建函数式API的方法，使用keras.models.Model(inputs = [],outputs = []),inputs和outputs参数中传入列表的方式表示多输入与多输出，layers仍然使用上述函数式API方式定义   </li><li>由于是多输出，可以针对不同的输出构造不同的损失函数，并对不同的损失函数赋予不同的权重以改变其对整体损失函数的影响<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    optimizer=keras.optimizers.RMSprop(<span class="number">1e-3</span>),</span><br><span class="line">    loss=[</span><br><span class="line">        keras.losses.CategoricalCrossEntropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">        keras.losses.BinaryCrossEntropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">    ],</span><br><span class="line">    loss_weights=[<span class="number">0.2</span>,<span class="number">0.8</span>],</span><br><span class="line">)<span class="comment"># 或者通过输出层的名称对loss进行构造：即loss参数使用字典定义 </span></span><br></pre></td></tr></table></figure></li><li>使用model.fit对模型进行训练时，训练集中的x和y是通过字典元组进行传参的  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(</span><br><span class="line">    &#123;<span class="string">&#x27;输入层名称1&#x27;</span>:对应numpy数组<span class="number">1</span>,<span class="string">&#x27;输入层名称2&#x27;</span>:对应numpy数组<span class="number">2</span>,...&#125;,<span class="comment"># 训练集中的x</span></span><br><span class="line">    &#123;<span class="string">&#x27;输出层名称1&#x27;</span>:对应numpy数组<span class="number">1</span>,<span class="string">&#x27;输出层名称2&#x27;</span>:对应numpy数组<span class="number">2</span>,...&#125;,</span><br><span class="line">    <span class="comment"># 训练集中的y</span></span><br><span class="line">    epochs=<span class="number">20</span>,</span><br><span class="line">    batch_size=<span class="number">10</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="函数式API实现ResNet模型"><a href="#函数式API实现ResNet模型" class="headerlink" title="函数式API实现ResNet模型"></a>函数式API实现ResNet模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = keras.Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), name=<span class="string">&quot;img&quot;</span>)</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(inputs)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">block_1_output = layers.MaxPooling2D(<span class="number">3</span>)(x)</span><br><span class="line"></span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>, padding=<span class="string">&quot;same&quot;</span>)(block_1_output)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>, padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line">block_2_output = layers.add([x, block_1_output])</span><br><span class="line"></span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>, padding=<span class="string">&quot;same&quot;</span>)(block_2_output)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>, padding=<span class="string">&quot;same&quot;</span>)(x)</span><br><span class="line">block_3_output = layers.add([x, block_2_output])</span><br><span class="line"></span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, activation=<span class="string">&quot;relu&quot;</span>)(block_3_output)</span><br><span class="line">x = layers.GlobalAveragePooling2D()(x)</span><br><span class="line">x = layers.Dense(<span class="number">256</span>, activation=<span class="string">&quot;relu&quot;</span>)(x)</span><br><span class="line">x = layers.Dropout(<span class="number">0.5</span>)(x)</span><br><span class="line">outputs = layers.Dense(<span class="number">10</span>)(x)</span><br><span class="line"></span><br><span class="line">model = keras.Model(inputs, outputs, name=<span class="string">&quot;toy_resnet&quot;</span>) </span><br></pre></td></tr></table></figure><h1 id="子类API"><a href="#子类API" class="headerlink" title="子类API"></a>子类API</h1><h2 id="初识子类API"><a href="#初识子类API" class="headerlink" title="初识子类API"></a>初识子类API</h2>keras的一个中心抽象是Layer层，层封装了状态（层的权重，对应__init__()函数定义）和从输入到输出的转换（调用，即层的前向传递，对应call()函数定义）<h2 id="权重的定义"><a href="#权重的定义" class="headerlink" title="权重的定义"></a>权重的定义</h2></li></ol><ul><li>通过初始化函数定义weight<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units = <span class="number">32</span>,input_dim = <span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Linear,self).__init__()</span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        self.w = tf.Variable(initial_value = w_init( <span class="comment"># 层的权重w</span></span><br><span class="line">            shape = (input_dim,units),  </span><br><span class="line">            dtype = <span class="string">&quot;float32&quot;</span>),  </span><br><span class="line">            trainable = <span class="literal">True</span>)  <span class="comment"># 是否可训练（反向传播时是否考虑）</span></span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line">        self.b = tf.Variable(initial_value = b_init( <span class="comment"># 层的权重b</span></span><br><span class="line">            shape =  (units,),  </span><br><span class="line">            dtype = <span class="string">&quot;float32&quot;</span>),  </span><br><span class="line">            trainable = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs,self.w)+self.b</span><br><span class="line"></span><br><span class="line">x = tf.ones(<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line">linear_layer = Linear(<span class="number">2</span>,<span class="number">8</span>) <span class="comment"># 创建layer对象</span></span><br><span class="line">output = linear_layer(x) <span class="comment">#调用call函数，对数据x进行前向传播</span></span><br></pre></td></tr></table></figure></li><li>通过tf.add_weight()函数定义weight<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units=<span class="number">32</span>,input_dim=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Linear,self).__init__()</span><br><span class="line">        self.w = tf.add_weight(  <span class="comment"># tf.add_weight()定义层的权重</span></span><br><span class="line">            shape=(input_dim,units),  </span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,  </span><br><span class="line">            trainable = <span class="literal">True</span>)</span><br><span class="line">        self.b = tf.add_weight(  </span><br><span class="line">            shape = (units,),  </span><br><span class="line">            initializer = <span class="string">&quot;zeros&quot;</span>,  </span><br><span class="line">            trainable = <span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span> (<span class="params">self,<span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(self.w,<span class="built_in">input</span>)+self.b</span><br><span class="line">x = tf.ones(<span class="number">10</span>,<span class="number">8</span>)</span><br><span class="line">linear_layer = Linear(<span class="number">2</span>,<span class="number">8</span>) <span class="comment"># 创建layer对象</span></span><br><span class="line">output = linear_layer(x) <span class="comment">#调用call函数，对数据x进行前向传播</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>将权重构建推迟到得知输入的形状之后：定义self.bulid()函数<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units = <span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Linear,self).__init__()</span><br><span class="line">        self.units = units</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self,input_shape</span>):</span></span><br><span class="line">        self.w = tf.add_weight(  </span><br><span class="line">            shape = (input_shape,self.units),  </span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,  </span><br><span class="line">            trainable = <span class="literal">True</span>)</span><br><span class="line">        self.b = tf.add_weight(  </span><br><span class="line">            shape = (self.units,),  </span><br><span class="line">            initializer = <span class="string">&quot;zeros&quot;</span>,  </span><br><span class="line">            trainable = <span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(self.w,inputs)+self.b</span><br><span class="line"></span><br><span class="line"><span class="comment"># At instantiation, we don&#x27;t know on what inputs this is going to get called</span></span><br><span class="line">linear_layer = Linear(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The layer&#x27;s weights are created dynamically the first time the layer is called</span></span><br><span class="line">y = linear_layer(x)</span><br></pre></td></tr></table></figure><h2 id="前向过程的定义"><a href="#前向过程的定义" class="headerlink" title="前向过程的定义"></a>前向过程的定义</h2></li><li>在call（）函数中通过self.add_loss(value)实现损失张量的定义，而且这些损失（包括内部层创建的损失）可以由layer.losses()取回<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span>  <span class="title">ActivityRegularizationLayer</span> (<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,rate = <span class="number">1e-2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActivityRegularizationLayer,self)</span><br><span class="line">        self.rate = rate</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs</span>):</span></span><br><span class="line">        self.add_loss(self.rate*tf.reduce_sum(inputs))</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br></pre></td></tr></table></figure>这些loss可以在model.fit()函数中自动求和并添加到主损失中</li><li>在call()函数中通过self.add_metric()方法添加度量指标，同add_loss（），同样可以通过layer.metrics访问获取，也可以通过fit()跟踪</li><li>可以通过在子类API中添加get_config()方法将自定义层作为函数式模型的一部分进行序列化</li><li>另外，基础 Layer 类的 <strong>init</strong>() 方法会接受一些关键字参数，尤其是 name 和 dtype。最好将这些参数传递给 <strong>init</strong>() 中的父类，并将其包含在层配置中：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units=<span class="number">32</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Linear, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        self.w = self.add_weight(</span><br><span class="line">            shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line">        self.b = self.add_weight(</span><br><span class="line">            shape=(self.units,), initializer=<span class="string">&quot;random_normal&quot;</span>, trainable=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span>(<span class="params">self</span>):</span></span><br><span class="line">        config = <span class="built_in">super</span>(Linear, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">&quot;units&quot;</span>: self.units&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">64</span>)</span><br><span class="line">config = layer.get_config()</span><br><span class="line"><span class="built_in">print</span>(config)</span><br><span class="line">new_layer = Linear.from_config(config)</span><br></pre></td></tr></table></figure></li><li>关于call()方法中的training特权参数<br>这个参数的目的就是为了区分目前是处于训练阶段(training == True)还是推断阶段(training == False),因为某些layer（如BatchNormalization层和Dropout层）在训练时和推断时有不同的行为</li></ul><h1 id="Model和Layer"><a href="#Model和Layer" class="headerlink" title="Model和Layer"></a>Model和Layer</h1><p>Model类和Layer类具有相同的API，但是Model类具有以下的特殊功能:</p><ul><li>公开内置训练、评估和预测循环：model.fit(),model.evaluate(),model.predict()</li><li>可以通过model.layers获取其内部的层结构；</li><li>公开保存和序列化API：save()以及save_weights()</li></ul>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 编程实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李宏毅老师2021春机器学习课程-CNN</title>
      <link href="/2021/11/27/machine-learning-hung-yi-CNN/"/>
      <url>/2021/11/27/machine-learning-hung-yi-CNN/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 李宏毅老师2021春季机器学习课程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 基础知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/31/hello-world/"/>
      <url>/2021/10/31/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
